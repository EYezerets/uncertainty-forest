{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
  },
  "orig_nbformat": 2,
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##notebook modified from rguo123: Richard Guo\n",
    "\n",
    "import mixed\n",
    "import time\n",
    "import pandas as pd\n",
    "import logging\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### UF CODE\n",
    "'''\n",
    "Primary Author: Will LeVine\n",
    "Email: levinewill@icloud.com\n",
    "'''\n",
    "#Model\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "#Infrastructure\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.utils.validation import NotFittedError\n",
    "\n",
    "#Data Handling\n",
    "from sklearn.utils.validation import (\n",
    "    check_X_y,\n",
    "    check_array,\n",
    "    NotFittedError,\n",
    ")\n",
    "from sklearn.utils.multiclass import check_classification_targets\n",
    "\n",
    "#Utils\n",
    "from joblib import Parallel, delayed\n",
    "import numpy as np\n",
    "\n",
    "def _finite_sample_correction(posteriors, num_points_in_partition, num_classes):\n",
    "    '''\n",
    "    encourage posteriors to approach uniform when there is low data\n",
    "    '''\n",
    "    correction_constant = 1 / (num_classes * num_points_in_partition)\n",
    "\n",
    "    zero_posterior_idxs = np.where(posteriors == 0)[0]\n",
    "    posteriors[zero_posterior_idxs] = correction_constant\n",
    "    \n",
    "    posteriors /= sum(posteriors)\n",
    "    \n",
    "    return posteriors\n",
    "\n",
    "class UncertaintyForest(BaseEstimator, ClassifierMixin):\n",
    "    '''\n",
    "    based off of https://arxiv.org/pdf/1907.00325.pdf\n",
    "    '''\n",
    "    def __init__(\n",
    "        self,\n",
    "        max_depth=30,\n",
    "        min_samples_leaf=1,\n",
    "        max_samples = 0.63,\n",
    "        max_features_tree = \"auto\",\n",
    "        n_estimators=100,\n",
    "        bootstrap=False,\n",
    "        parallel=True,\n",
    "        n_jobs = None):\n",
    "\n",
    "        #Tree parameters.\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.max_features_tree = max_features_tree\n",
    "\n",
    "        #Bag parameters\n",
    "        self.n_estimators = n_estimators\n",
    "        self.bootstrap = bootstrap\n",
    "        self.max_samples = max_samples\n",
    "\n",
    "        #Model parameters.\n",
    "        self.parallel = parallel\n",
    "        if self.parallel and n_jobs == None:\n",
    "            self.n_jobs = self.n_estimators\n",
    "        else:\n",
    "            self.n_jobs = n_jobs\n",
    "        self.fitted = False\n",
    "\n",
    "    def _check_fit(self):\n",
    "        '''\n",
    "        raise a NotFittedError if the model isn't fit\n",
    "        '''\n",
    "        if not self.fitted:\n",
    "                msg = (\n",
    "                        \"This %(name)s instance is not fitted yet. Call 'fit' with \"\n",
    "                        \"appropriate arguments before using this estimator.\"\n",
    "                )\n",
    "                raise NotFittedError(msg % {\"name\": type(self).__name__})\n",
    "\n",
    "    def transform(self, X):\n",
    "        '''\n",
    "        get the estimated posteriors across trees\n",
    "        '''\n",
    "        X = check_array(X)\n",
    "                \n",
    "        def worker(tree_idx, tree):\n",
    "            #get the nodes of X\n",
    "            # Drop each estimation example down the tree, and record its 'y' value.\n",
    "            return tree.apply(X)\n",
    "            \n",
    "\n",
    "        if self.parallel:\n",
    "            return np.array(\n",
    "                    Parallel(n_jobs=self.n_jobs)(\n",
    "                            delayed(worker)(tree_idx, tree) for tree_idx, tree in enumerate(self.ensemble.estimators_)\n",
    "                    )\n",
    "            )         \n",
    "        else:\n",
    "            return np.array(\n",
    "                    [worker(tree_idx, tree) for tree_idx, tree in enumerate(self.ensemble.estimators_)]\n",
    "                    )\n",
    "        \n",
    "    def get_transformer(self):\n",
    "        return lambda X : self.transform(X)\n",
    "        \n",
    "    def vote(self, nodes_across_trees):\n",
    "        return self.voter.predict(nodes_across_trees)\n",
    "        \n",
    "    def get_voter(self):\n",
    "        return self.voter\n",
    "        \n",
    "                        \n",
    "    def fit(self, X, y):\n",
    "\n",
    "        #format X and y\n",
    "        X, y = check_X_y(X, y)\n",
    "        check_classification_targets(y)\n",
    "        self.classes_, y = np.unique(y, return_inverse=True)\n",
    "        \n",
    "        #define the ensemble\n",
    "        self.ensemble = BaggingClassifier(\n",
    "            DecisionTreeClassifier(\n",
    "                max_depth=self.max_depth,\n",
    "                min_samples_leaf=self.min_samples_leaf,\n",
    "                max_features=self.max_features_tree\n",
    "            ),\n",
    "            n_estimators=self.n_estimators,\n",
    "            max_samples=self.max_samples,\n",
    "            bootstrap=self.bootstrap,\n",
    "            n_jobs = self.n_jobs\n",
    "        )\n",
    "        \n",
    "        #fit the ensemble\n",
    "        self.ensemble.fit(X, y)\n",
    "        \n",
    "        class Voter(BaseEstimator):\n",
    "            def __init__(self, estimators_samples_, classes, parallel, n_jobs):\n",
    "                self.n_estimators = len(estimators_samples_)\n",
    "                self.classes_ = classes\n",
    "                self.parallel = parallel\n",
    "                self.estimators_samples_ = estimators_samples_\n",
    "                self.n_jobs = n_jobs\n",
    "            \n",
    "            def fit(self, nodes_across_trees, y, fitting = False):\n",
    "                self.tree_idx_to_node_ids_to_posterior_map = {}\n",
    "\n",
    "                def worker(tree_idx):\n",
    "                    nodes = nodes_across_trees[tree_idx]\n",
    "                    oob_samples = np.delete(range(len(nodes)), self.estimators_samples_[tree_idx])\n",
    "                    cal_nodes = nodes[oob_samples] if fitting else nodes\n",
    "                    y_cal = y[oob_samples] if fitting else y                    \n",
    "                    \n",
    "                    #create a map from the unique node ids to their classwise posteriors\n",
    "                    node_ids_to_posterior_map = {}\n",
    "\n",
    "                    #fill in the posteriors \n",
    "                    for node_id in np.unique(cal_nodes):\n",
    "                        cal_idxs_of_node_id = np.where(cal_nodes == node_id)[0]\n",
    "                        cal_ys_of_node = y_cal[cal_idxs_of_node_id]\n",
    "                        class_counts = [len(np.where(cal_ys_of_node == y)[0]) for y in np.unique(y) ]\n",
    "                        posteriors = np.nan_to_num(np.array(class_counts) / np.sum(class_counts))\n",
    "\n",
    "                        #finite sample correction\n",
    "                        posteriors_corrected = _finite_sample_correction(posteriors, len(cal_idxs_of_node_id), len(self.classes_))\n",
    "                        node_ids_to_posterior_map[node_id] = posteriors_corrected\n",
    "\n",
    "                    #add the node_ids_to_posterior_map to the overall tree_idx map \n",
    "                    self.tree_idx_to_node_ids_to_posterior_map[tree_idx] = node_ids_to_posterior_map\n",
    "                    \n",
    "                for tree_idx in range(self.n_estimators):\n",
    "                        worker(tree_idx)\n",
    "                return self\n",
    "                        \n",
    "                        \n",
    "            def predict_proba(self, nodes_across_trees):\n",
    "                def worker(tree_idx):\n",
    "                    #get the node_ids_to_posterior_map for this tree\n",
    "                    node_ids_to_posterior_map = self.tree_idx_to_node_ids_to_posterior_map[tree_idx]\n",
    "\n",
    "                    #get the nodes of X\n",
    "                    nodes = nodes_across_trees[tree_idx]\n",
    "\n",
    "                    posteriors = []\n",
    "                    node_ids = node_ids_to_posterior_map.keys()\n",
    "\n",
    "                    #loop over nodes of X\n",
    "                    for node in nodes:\n",
    "                        #if we've seen this node before, simply get the posterior\n",
    "                        if node in node_ids:\n",
    "                            posteriors.append(node_ids_to_posterior_map[node])\n",
    "                        #if we haven't seen this node before, simply use the uniform posterior \n",
    "                        else:\n",
    "                            posteriors.append(np.ones((len(np.unique(self.classes_)))) / len(self.classes_))\n",
    "                    return posteriors\n",
    "\n",
    "                if self.parallel:\n",
    "                    return np.mean(\n",
    "                            Parallel(n_jobs=self.n_jobs)(\n",
    "                                    delayed(worker)(tree_idx) for tree_idx in range(self.n_estimators)\n",
    "                            ), axis = 0\n",
    "                    )\n",
    "\n",
    "                else:\n",
    "                    return np.mean(\n",
    "                            [worker(tree_idx) for tree_idx in range(self.n_estimators)], axis = 0)\n",
    "                \n",
    "        #get the nodes of the calibration set\n",
    "        nodes_across_trees = self.transform(X) \n",
    "        self.voter = Voter(estimators_samples_ = self.ensemble.estimators_samples_, classes = self.classes_, parallel = self.parallel, n_jobs = self.n_jobs)\n",
    "        self.voter.fit(nodes_across_trees, y, fitting = True)\n",
    "        self.fitted = True\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.classes_[np.argmax(self.predict_proba(X), axis=-1)]\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return self.voter.predict_proba(self.transform(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_big_data(n_samples=10000, n_features=100):\n",
    "    X = np.random.rand(n_samples, n_features)\n",
    "    y = np.random.binomial(1, .5, n_samples)\n",
    "    return X, np.ravel(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_samples = 1000 #10000000 \n",
    "# n_features = 10 #50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X, y = generate_big_data(n_samples=n_samples, n_features=n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UncertaintyForest(n_estimators = 10, parallel=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "irf = CalibratedClassifierCV(base_estimator=RandomForestClassifier(n_estimators = 10), \n",
    "                                     method='isotonic', \n",
    "                                     cv = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uf_elapsed_varysamples = []\n",
    "uf_elapsed_varyfeatures = []\n",
    "irf_elapsed_varysamples = []\n",
    "irf_elapsed_varyfeatures = []\n",
    "ksg_elapsed_varysamples = []\n",
    "ksg_elapsed_varyfeatures = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_range = range(100,1000000, 50000)\n",
    "feature_range = range(1, 5000, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in sample_range: #(1,1000000,100000)\n",
    "    X, y = generate_big_data(n_samples=i, n_features=10)\n",
    "\n",
    "    start = time.time()\n",
    "    mixed.KSG(X, y)\n",
    "    end = time.time()\n",
    "    ksg_elapsed = end - start\n",
    "    print(ksg_elapsed)\n",
    "    logging.basicConfig(filename='logs.log', level=logging.INFO)\n",
    "    logging.info('ksg samples: {}, {}'.format(i,ksg_elapsed))\n",
    "    ksg_elapsed_varysamples.append(ksg_elapsed)\n",
    "\n",
    "    start = time.time()\n",
    "    model.fit(X, y)\n",
    "    end = time.time()\n",
    "    uf_elapsed = end - start\n",
    "    print(uf_elapsed)\n",
    "    logging.basicConfig(filename='logs.log', level=logging.INFO)\n",
    "    logging.info('uf samples: {}, {}'.format(i,uf_elapsed))\n",
    "    uf_elapsed_varysamples.append(uf_elapsed)\n",
    "\n",
    "    start = time.time()\n",
    "    irf.fit(X, y) \n",
    "    end = time.time()\n",
    "    irf_elapsed = end - start\n",
    "    print(irf_elapsed)\n",
    "    logging.basicConfig(filename='logs.log', level=logging.INFO)\n",
    "    logging.info('irf samples: {}, {}'.format(i,irf_elapsed))\n",
    "    irf_elapsed_varysamples.append(irf_elapsed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in feature_range:\n",
    "    X, y = generate_big_data(n_samples=10000, n_features=i) #100000\n",
    "\n",
    "    start = time.time()\n",
    "    model.fit(X, y)\n",
    "    end = time.time()\n",
    "    uf_elapsed = end - start\n",
    "    print(uf_elapsed)\n",
    "    logging.basicConfig(filename='logs.log', level=logging.INFO)\n",
    "    logging.info('uf features: {}, {}'.format(i,uf_elapsed))\n",
    "    uf_elapsed_varyfeatures.append(uf_elapsed)\n",
    "\n",
    "    start = time.time()\n",
    "    mixed.KSG(X, y)\n",
    "    end = time.time()\n",
    "    ksg_elapsed = end - start\n",
    "    print(ksg_elapsed)\n",
    "    logging.basicConfig(filename='logs.log', level=logging.INFO)\n",
    "    logging.info('ksg features: {}, {}'.format(i,ksg_elapsed))\n",
    "    ksg_elapsed_varyfeatures.append(ksg_elapsed)\n",
    "\n",
    "    start = time.time()\n",
    "    irf.fit(X, y) \n",
    "    end = time.time()\n",
    "    irf_elapsed = end - start\n",
    "    print(irf_elapsed)\n",
    "    logging.basicConfig(filename='logs.log', level=logging.INFO)\n",
    "    logging.info('irf features: {}, {}'.format(i,irf_elapsed))\n",
    "    irf_elapsed_varyfeatures.append(irf_elapsed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start = time.time()\n",
    "# mixed.KSG(X, y)\n",
    "# end = time.time()\n",
    "# ksg_elapsed = end - start\n",
    "# print(ksg_elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stitch it together in a dataframe.\n",
    "# runtime_dict = {\n",
    "#     \"n_samples\": n_samples,\n",
    "#     \"n_features\": n_features,\n",
    "#     \"uf\": uf_elapsed,\n",
    "#     \"ksg\": ksg_elapsed\n",
    "# }\n",
    "\n",
    "# table = pd.DataFrame(runtime_dict, index=[\"values\"])\n",
    "# table.to_csv(\"runtime_table.csv\", index = False)\n",
    "# print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"logs.log\") as log:\n",
    "            print(log.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(uf_elapsed_varysamples)\n",
    "print(ksg_elapsed_varysamples)\n",
    "print(irf_elapsed_varysamples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(uf_elapsed_varyfeatures)\n",
    "print(ksg_elapsed_varyfeatures)\n",
    "print(irf_elapsed_varyfeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tvsamples(ax):\n",
    "    linewidth = 8\n",
    "    opacity = 1\n",
    "    ax.set_title('Time vs. Samples')\n",
    "\n",
    "    if len(sample_range) == len(uf_elapsed_varysamples):\n",
    "        ax.plot(sample_range, uf_elapsed_varysamples, \n",
    "                label = 'UF',\n",
    "                linewidth = linewidth, \n",
    "                color = '#F41711', \n",
    "                alpha = opacity)\n",
    "    if len(sample_range) == len(ksg_elapsed_varysamples):\n",
    "        ax.plot(sample_range, ksg_elapsed_varysamples, \n",
    "                label = 'KSG',\n",
    "                linewidth = linewidth, \n",
    "                color = '#1b9e77', \n",
    "                alpha = opacity)\n",
    "\n",
    "    if len(sample_range) == len(irf_elapsed_varysamples):\n",
    "        ax.plot(sample_range, irf_elapsed_varysamples, \n",
    "                label = 'IRF',\n",
    "                linewidth = linewidth, \n",
    "                color = '#fdae61', \n",
    "                alpha = opacity)      \n",
    "\n",
    "def plot_tvfeatures(ax):\n",
    "    linewidth = 8\n",
    "    opacity = 1\n",
    "    ax.set_title('Time vs. Features')\n",
    "    if len(feature_range) == len(uf_elapsed_varyfeatures):\n",
    "        ax.plot(feature_range, uf_elapsed_varyfeatures, \n",
    "                label = 'UF',\n",
    "                linewidth = linewidth, \n",
    "                color = '#F41711', \n",
    "                alpha = opacity)\n",
    "    if len(feature_range) == len(ksg_elapsed_varyfeatures):\n",
    "        ax.plot(feature_range, ksg_elapsed_varyfeatures, \n",
    "                label = 'KSG',\n",
    "                linewidth = linewidth, \n",
    "                color = '#1b9e77', \n",
    "                alpha = opacity)\n",
    "\n",
    "    if len(feature_range) == len(irf_elapsed_varyfeatures):\n",
    "        ax.plot(feature_range, irf_elapsed_varyfeatures, \n",
    "                label = 'IRF',\n",
    "                linewidth = linewidth, \n",
    "                color = '#fdae61', \n",
    "                alpha = opacity)    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_timefigs():\n",
    "    sns.set(font_scale = 4)\n",
    "    sns.set_style(\"ticks\")\n",
    "    plt.rcParams['figure.figsize'] = [28, 28]\n",
    "    fig, axes = plt.subplots(2, 1)\n",
    "    axes[0].set_ylim(-0.05, 2000.0)\n",
    "    axes[1].set_ylim(-0.05, 2000.0)\n",
    "    \n",
    "    axes[0].set_xlim(sample_range[0], sample_range[1])\n",
    "    axes[1].set_xlim(feature_range[0],feature_range[1])\n",
    "    axes[0].xaxis.set_ticks(sample_range)\n",
    "    axes[1].xaxis.set_ticks(feature_range)\n",
    "\n",
    "    axes[0].yaxis.set_ticks(np.arange(0.0, 2000.0, 200.0))\n",
    "    axes[1].yaxis.set_ticks(np.arange(0.0, 2000.0, 200.0))\n",
    "\n",
    "    plot_tvsamples(axes[0])\n",
    "    plot_tvfeatures(axes[1])\n",
    "\n",
    "    axes[0].set_ylabel(\"Time (s)\")\n",
    "    axes[1].set_ylabel(\"Time (s)\")\n",
    "    axes[0].set_xlabel(\"Samples (Features = 10)\")\n",
    "    axes[1].set_xlabel(\"Features (Samples = 10000)\")\n",
    "\n",
    "    axes[0].legend(loc = \"upper left\")\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.savefig(\"fig_timing.pdf\")\n",
    "    plt.show()\n",
    "    \n",
    "plot_timefigs()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}
