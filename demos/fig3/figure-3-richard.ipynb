{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import copy\n",
    "\n",
    "import npeet.entropy_estimators as ee\n",
    "from uncertainty_forest.uncertainty_forest import UncertaintyForest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "from scipy.stats import entropy, multivariate_normal\n",
    "from scipy.integrate import nquad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys,inspect\n",
    "current_dir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "sys.path.insert(0, parent_dir) \n",
    "import mixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taken from Richard's \"Reprod Figure 2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble.forest import _generate_unsampled_indices\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cef_estimate(X, y, n_estimators = 200, max_samples = .32, bootstrap = True, depth = 30, min_samples_leaf = 1, max_features = 1.):\n",
    "    model = BaggingClassifier(DecisionTreeClassifier(max_depth = depth, min_samples_leaf = min_samples_leaf, max_features = math.ceil(int(math.sqrt(X.shape[1])))), \n",
    "                              n_estimators = n_estimators, \n",
    "                              max_samples= max_samples, \n",
    "                              bootstrap = bootstrap)\n",
    "    model.fit(X, y)\n",
    "    class_counts = np.zeros((X.shape[0], model.n_classes_))\n",
    "    for tree in model:\n",
    "        # get out of bag indicies\n",
    "        \n",
    "        # RONAK EDIT STARTS HERE ################ In newer sklearn, generate unsampled takes a positional argument.\n",
    "        #unsampled_indices = _generate_unsampled_indices(tree.random_state, len(X))\n",
    "        unsampled_indices = _generate_unsampled_indices(tree.random_state, len(X), int((1 - max_samples)*len(X)))\n",
    "        # RONAK EDIT ENDS HERE ##################\n",
    "        \n",
    "        total_unsampled = len(unsampled_indices)\n",
    "        np.random.shuffle(unsampled_indices)\n",
    "        prob_indices, eval_indices = unsampled_indices[:total_unsampled//2], unsampled_indices[total_unsampled//2:]\n",
    "        # get all node counts\n",
    "        node_counts = tree.tree_.n_node_samples\n",
    "        # get probs for eval samples\n",
    "        posterior_class_counts = np.zeros((len(node_counts), model.n_classes_))\n",
    "        for prob_index in prob_indices:\n",
    "            posterior_class_counts[tree.apply(X[prob_index].reshape(1, -1)).item(), y[prob_index]] += 1\n",
    "        row_sums = posterior_class_counts.sum(axis=1)\n",
    "        row_sums[row_sums == 0] = 1\n",
    "        class_probs = (posterior_class_counts/row_sums[:, None])\n",
    "        \n",
    "        where_0 = np.argwhere(class_probs == 0)\n",
    "        for elem in where_0:\n",
    "            class_probs[elem[0], elem[1]] = 1/(2*row_sums[elem[0], None])\n",
    "        where_1 = np.argwhere(class_probs == 1)\n",
    "        for elem in where_1:\n",
    "            class_probs[elem[0], elem[1]] = 1 - 1/(2*row_sums[elem[0], None])\n",
    "        \n",
    "        class_probs.tolist()\n",
    "        partition_counts = np.asarray([node_counts[x] for x in tree.apply(X[eval_indices])])\n",
    "        # get probability for out of bag samples\n",
    "        eval_class_probs = [class_probs[x] for x in tree.apply(X[eval_indices])]\n",
    "        eval_class_probs = np.array(eval_class_probs)\n",
    "        # find total elements for out of bag samples\n",
    "        elems = np.multiply(eval_class_probs, partition_counts[:, np.newaxis])\n",
    "        # store counts for each x (repeat fhis for each tree)\n",
    "        class_counts[eval_indices] += elems\n",
    "    # calculate p(y|X = x) for all x's\n",
    "    probs = class_counts/class_counts.sum(axis = 1, keepdims = True)\n",
    "    entropies = -np.sum(np.log(probs)*probs, axis = 1)\n",
    "    # convert nan to 0\n",
    "    entropies = np.nan_to_num(entropies)\n",
    "    return np.mean(entropies)\n",
    "\n",
    "np.warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def CART_estimate(X, y, n_trees = 300, bootstrap = True, depth = 30):\n",
    "#     model = RandomForestClassifier(bootstrap = bootstrap, n_estimators =n_trees, max_depth = depth, max_features = math.ceil(int(math.sqrt(X.shape[1]))))\n",
    "#     model.fit(X, y)\n",
    "#     class_counts = np.zeros((X.shape[0], model.n_classes_))\n",
    "#     for tree_in_forest in model:\n",
    "#         # get number of training elements in each partition\n",
    "#         node_counts = tree_in_forest.tree_.n_node_samples\n",
    "#         # get counts for all x (x.length array)\n",
    "#         partition_counts = np.asarray([node_counts[x] for x in tree_in_forest.apply(X)])\n",
    "#         # get class probability for all x (x.length, n_classes)\n",
    "#         class_probs = tree_in_forest.predict_proba(X)\n",
    "#         # get elements by performing row wise multiplication\n",
    "#         elems = np.multiply(class_probs, partition_counts[:, np.newaxis])\n",
    "#         # update counts for that tree\n",
    "#         class_counts += elems\n",
    "#     probs = class_counts/class_counts.sum(axis=1, keepdims=True)\n",
    "#     entropies = -np.sum(np.log(probs)*probs, axis = 1)\n",
    "#     # convert nan to 0\n",
    "#     entropies = np.nan_to_num(entropies)\n",
    "#     return np.mean(entropies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating data and prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def split_train_eval(X, y, frac_eval):\n",
    "    \n",
    "#     if frac_eval == 0:\n",
    "#         return X, y, [], []\n",
    "    \n",
    "#     n = len(y)\n",
    "#     n_eval = int(np.floor(frac_eval*n))\n",
    "#     eval_indices = np.random.choice(np.arange(n), size = n_eval, replace = False)\n",
    "#     X_eval = X[eval_indices, :]\n",
    "#     y_eval = y[eval_indices]\n",
    "#     X = np.delete(X, eval_indices, axis = 0)\n",
    "#     y = np.delete(y, eval_indices, axis = 0)\n",
    "    \n",
    "#     return X, y, X_eval, y_eval\n",
    "\n",
    "def generate_data(n, d, mu = 1, var1 = 1, pi = 0.5, truncate = False, three_class = False):\n",
    "    \n",
    "    means, Sigma, probs = _make_params(d, mu = mu, var1 = var1, pi = pi, three_class = three_class)\n",
    "    counts = np.random.multinomial(n, probs, size = 1)[0]\n",
    "    \n",
    "    X_data = []\n",
    "    y_data = []\n",
    "    for k in range(len(probs)):\n",
    "        X_data.append(np.random.multivariate_normal(means[k], Sigma, counts[k]))\n",
    "        y_data.append(np.repeat(k, counts[k]))\n",
    "    X = np.concatenate(tuple(X_data))\n",
    "    y = np.concatenate(tuple(y_data))\n",
    "    \n",
    "    if truncate:\n",
    "        for i in range(n):\n",
    "            if X[i, 0] > 0:\n",
    "                y[i] = 0\n",
    "            elif X[i, 0] < 0:\n",
    "                y[i] = 1\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "def _make_params(d, mu = 1, var1 = 1, pi = 0.5, three_class = False):\n",
    "    \n",
    "    if three_class:\n",
    "        return _make_three_class_params(d, mu)\n",
    "    \n",
    "    mean = np.zeros(d)\n",
    "    mean[0] = mu\n",
    "    means = [mean, -mean]\n",
    "    Sigma = np.eye(d)\n",
    "    Sigma[0, 0] = var1\n",
    "    probs = [pi, 1 - pi]\n",
    "    \n",
    "    return means, Sigma, probs\n",
    "\n",
    "def _make_three_class_params(d, mu):\n",
    "    \n",
    "    means = []\n",
    "    mean = np.zeros(d)\n",
    "    \n",
    "    mean[0] = mu\n",
    "    means.append(copy.deepcopy(mean))\n",
    "    \n",
    "    mean[0] = -mu\n",
    "    means.append(copy.deepcopy(mean))\n",
    "    \n",
    "    mean[0] = 0\n",
    "    mean[d-1] = mu\n",
    "    means.append(copy.deepcopy(mean))\n",
    "    \n",
    "    Sigma = np.eye(d)\n",
    "    probs = [1/3.]*3\n",
    "    \n",
    "    return means, Sigma, probs\n",
    "\n",
    "# def split_by_class(X, y):\n",
    "    \n",
    "#     classes, class_indices = np.unique(y, return_inverse = True)\n",
    "#     K = len(classes)\n",
    "#     X_by_class = []\n",
    "#     y_by_class = []\n",
    "    \n",
    "#     for k in range(K):\n",
    "#         class_ = classes[k]\n",
    "#         X_k = X[y == class_,:]\n",
    "#         y_k = np.repeat(class_, X_k.shape[0])\n",
    "#         X_by_class.append(X_k)\n",
    "#         y_by_class.append(y_k)\n",
    "        \n",
    "#     return X_by_class, y_by_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the Setting (\"Look at it!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_setting(n, setting, ax):\n",
    "    \n",
    "    mean = 3 if setting['name'] == 'Three Class Gaussians' else 1\n",
    "    X, y = generate_data(n, 2, **setting['kwargs'], mu = mean)\n",
    "        \n",
    "    # X_by_class, y_by_class = split_by_class(X, y)\n",
    "    colors = [\"#c51b7d\", \"#2166ac\", \"#d95f02\"]\n",
    "    ax.scatter(X[:, 0], X[:, 1], color = np.array(colors)[y], marker = \".\")\n",
    "    \n",
    "    # ax.set_ylim(bottom = -5.05)\n",
    "    # ax.set_ylim(top = 5.05)\n",
    "    ax.set_xlim(left = -5.05)\n",
    "    ax.set_xlim(right = 5.05)\n",
    "    \n",
    "    ax.set_ylabel(setting['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting figures.\n",
    "# colors = [\"#d95f02\", \"#2166ac\", \"#c51b7d\"]\n",
    "settings = [\n",
    "    {\n",
    "        'name' : 'Sphereical Gaussians',\n",
    "        'kwargs': {},\n",
    "        # 'colors' : colors[1:3],\n",
    "        'filename' : 'spherical'\n",
    "    },\n",
    "    {\n",
    "        'name' : 'Elliptical Gaussians',\n",
    "        'kwargs': {'var1' : 3},\n",
    "        # 'colors' : colors[1:3],\n",
    "        'filename' : 'ellyptical'\n",
    "    },\n",
    "    {\n",
    "        'name' : 'Imbalanced Classes',\n",
    "        'kwargs': {'pi' : 0.8},\n",
    "        # 'colors' : colors[1:3],\n",
    "        'filename' : 'imbalanced'\n",
    "    },\n",
    "    {\n",
    "        'name' : 'Truncated Gaussians',\n",
    "        'kwargs': {'truncate' : True},\n",
    "        # 'colors' : colors[1:3],\n",
    "        'filename' : 'truncated'\n",
    "    },\n",
    "    {\n",
    "        'name' : 'Three Class Gaussians',\n",
    "        'kwargs': {'three_class' : True},\n",
    "        # 'colors' : colors,\n",
    "        'filename' : 'three_class'\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot data.\n",
    "fig, axes = plt.subplots(1, len(settings), figsize = (22,4))\n",
    "for i, setting in enumerate(settings):\n",
    "    plot_setting(2000, setting, axes[i])\n",
    "    \n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def estimate_mi(X, y, label, obj, frac_eval, base = 2):\n",
    "#     if label == 'UF':\n",
    "#         X, y, X_eval, y_eval = split_train_eval(X, y, frac_eval)\n",
    "#         obj.fit(X, y)\n",
    "#         return obj.estimate_mutual_info(X_eval)\n",
    "#     elif label == 'KSG':\n",
    "#         return ee.mi(X, np.array(y).reshape(-1, 1))\n",
    "#     elif label == 'Mixed KSG':\n",
    "#         return mixed.Mixed_KSG(X, y.reshape(-1, 1))\n",
    "#     elif label == 'IRF':\n",
    "#         X, y, X_eval, y_eval = split_train_eval(X, y, frac_eval)\n",
    "#         obj.fit(X, y)\n",
    "        \n",
    "#         _, counts = np.unique(y, return_counts=True)\n",
    "#         H_Y = entropy(counts, base=base)\n",
    "        \n",
    "#         p = obj.predict_proba(X_eval)\n",
    "#         H_YX = np.mean(entropy(p.T, base=base))\n",
    "#         return H_Y - H_YX\n",
    "#     else:\n",
    "#         raise ValueError(\"Uncrecognized label!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Effect Size Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mutual_info(d, base = np.exp(1), mu = 1, var1 = 1, pi = 0.5, three_class = False, truncate = False):\n",
    "    \n",
    "    if truncate:\n",
    "        return 1.0, 1.0, 1.0\n",
    "    \n",
    "    if d > 1:\n",
    "        dim = 2\n",
    "    else:\n",
    "        dim = d\n",
    " \n",
    "    means, Sigma, probs = _make_params(dim, mu = mu, var1 = var1, pi = pi, three_class = three_class)\n",
    "    \n",
    "    # Compute entropy and X and Y.\n",
    "    def func(*args):\n",
    "        x = np.array(args)\n",
    "        p = 0\n",
    "        for k in range(len(means)):\n",
    "            p += probs[k] * multivariate_normal.pdf(x, means[k], Sigma)\n",
    "        return -p * np.log(p) / np.log(base)\n",
    "\n",
    "    scale = 10\n",
    "    lims = [[-scale, scale]]*dim\n",
    "    H_X, int_err = nquad(func, lims)\n",
    "    H_Y = entropy(probs, base = base)\n",
    "    \n",
    "    # Compute MI.\n",
    "    H_XY = (dim * np.log(2*np.pi) + np.log(np.linalg.det(Sigma)) + dim) / (2 * np.log(base))\n",
    "    I_XY = H_X - H_XY\n",
    "    \n",
    "    return I_XY, H_X, H_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mutual_info_vs_mu(n, d, num_trials, mus, setting, frac_eval, base = np.exp(1)):\n",
    "    \n",
    "    uf = np.zeros((len(mus), num_trials))\n",
    "    # cart = np.zeros((len(mus), num_trials))\n",
    "    irf = np.zeros((len(mus), num_trials))\n",
    "    ksg = np.zeros((len(mus), num_trials))\n",
    "    mksg = np.zeros((len(mus), num_trials))\n",
    "    \n",
    "    def worker(t):\n",
    "        X, y = generate_data(n, d, mu = elem, **setting['kwargs'])\n",
    "        I_XY, H_X, H_Y = compute_mutual_info(d, mu = elem, **setting['kwargs'])\n",
    "        norm_factor = min(H_X, H_Y)\n",
    "        \n",
    "        # UF\n",
    "        _, counts = np.unique(y, return_counts=True)\n",
    "        est_H_Y = entropy(counts, base=np.exp(1))\n",
    "        uf_out = (est_H_Y - cef_estimate(np.array(X), y, 300, .32, depth = 30)) / norm_factor\n",
    "        \n",
    "        # CART\n",
    "        # cart_out = CART_estimate(X, y)\n",
    "        \n",
    "        # IRF\n",
    "        irf_obj = CalibratedClassifierCV(base_estimator=RandomForestClassifier(n_estimators = 300), \n",
    "                                     method='isotonic', \n",
    "                                     cv = 5)\n",
    "        X_train, X_eval, y_train, y_eval = train_test_split(X, y, test_size = frac_eval)\n",
    "        irf_obj.fit(X_train, y_train)\n",
    "        p = irf_obj.predict_proba(X_eval)\n",
    "        irf_out = np.mean(entropy(p.T, base = base)) / norm_factor\n",
    "        \n",
    "        # KSG\n",
    "        ksg_out = ee.mi(X, np.array(y).reshape(-1, 1)) / norm_factor\n",
    "        \n",
    "        # MKSG\n",
    "        mksg_out = mixed.Mixed_KSG(X, y.reshape(-1, 1)) / norm_factor\n",
    "        \n",
    "        return (uf_out, irf_out, ksg_out, mksg_out)\n",
    "    \n",
    "    for i, elem in enumerate(mus):\n",
    "        output = np.array(Parallel(n_jobs=-2)(delayed(worker)(t) for t in range(num_trials)))\n",
    "        uf[i, :] = output[:, 0]\n",
    "        # cart[i, :] = output[:, 1]\n",
    "        irf[i, :] = output[:, 1]\n",
    "        ksg[i, :] = output[:, 2]\n",
    "        mksg[i, :] = output[:, 3]\n",
    "        \n",
    "    pickle.dump(mus, open('mus.pkl', 'wb'))\n",
    "    pickle.dump(uf, open('uf_by_mu_d_%d_%s.pkl' % (d, setting['filename']), 'wb'))\n",
    "    # pickle.dump(cart, open('cart_by_mu_d_%d_%s.pkl' % (d, setting['filename']), 'wb'))\n",
    "    pickle.dump(irf, open('irf_by_mu_d_%d_%s.pkl' % (d, setting['filename']), 'wb'))\n",
    "    pickle.dump(ksg, open('ksg_by_mu_d_%d_%s.pkl' % (d, setting['filename']), 'wb'))\n",
    "    pickle.dump(mksg, open('mksg_by_mu_d_%d_%s.pkl' % (d, setting['filename']), 'wb'))\n",
    "\n",
    "    return uf, irf, ksg, mksg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def mutual_info_distribution(n, d, frac_eval, algos, num_trials, setting, mu, base = 2, parallel = False):\n",
    "    \n",
    "#     # Compute normalizing factors.\n",
    "#     kwargs = copy.deepcopy(setting['kwargs'])\n",
    "#     kwargs['mu'] = mu\n",
    "#     _, H_X, H_Y = compute_norm_mutual_info(base = base, **kwargs)\n",
    "#     factor = min(H_X, H_Y)\n",
    "    \n",
    "#     # For each trial, generate data and compute conditional entropy for each algorithm.\n",
    "#     def worker(t):\n",
    "#         X, y = generate_data(n, d, **kwargs)\n",
    "#         X, y, X_eval, _ = split_train_eval(X, y, frac_eval)\n",
    "        \n",
    "#         ret = np.zeros(len(algos))\n",
    "#         for j in range(len(algos)):\n",
    "#             # Estimate conditional probability of Y | X.\n",
    "#             ret[j] = estimate_mi(X, y, algos[j]['label'], algos[j]['instance'], frac_eval) / factor\n",
    "#         return ret\n",
    "    \n",
    "#     if parallel:\n",
    "#         predicted_mutual_info = np.array(Parallel(n_jobs=-2)(delayed(worker)(t) for t in range(num_trials)))\n",
    "#     else:\n",
    "#         predicted_mutual_info = np.zeros((num_trials, len(algos)))\n",
    "#         for t in tqdm(range(num_trials)):\n",
    "#             predicted_mutual_info[t, :] = worker(t)\n",
    "            \n",
    "#     return predicted_mutual_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def mutual_info_by_mu(mus, n, d, frac_eval, algos, num_trials, setting, parallel = False):\n",
    "#     # Repeat for all 'mu', and save output in the 'algos' array.\n",
    "#     mutual_info_range = np.zeros((len(mus), num_trials, len(algos)))\n",
    "#     for i in range(len(mus)):\n",
    "#         mutual_info_range[i, :, :] = mutual_info_distribution(n, \n",
    "#                                                               d, \n",
    "#                                                               frac_eval, \n",
    "#                                                               algos, \n",
    "#                                                               num_trials,\n",
    "#                                                               setting,\n",
    "#                                                               mus[i], \n",
    "#                                                               parallel = parallel)      \n",
    "#     for j in range(len(algos)):\n",
    "#         algos[j]['mi_by_mu_d_%d_%s' % (d, setting['filename'])] = mutual_info_range[:, :, j]\n",
    "        \n",
    "#     with open('algos_fig3.pkl', 'wb') as f:\n",
    "#         pickle.dump(algos, f)\n",
    "#     with open('mus_fig3.pkl', 'wb') as f:\n",
    "#         pickle.dump(mus, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mutual_info_by_mu(setting, algos, d, ax):\n",
    "    \n",
    "    mus = pickle.load(open('mus.pkl', 'rb'))\n",
    "    uf = pickle.load(open('uf_by_mu_d_%d_%s.pkl' % (d, setting['filename']), 'rb'))\n",
    "    # cart = pickle.load(open('cart_by_mu_d_%d_%s.pkl' % (d, setting['filename']), 'rb'))\n",
    "    irf = pickle.load(open('irf_by_mu_d_%d_%s.pkl' % (d, setting['filename']), 'rb'))\n",
    "    ksg = pickle.load(open('ksg_by_mu_d_%d_%s.pkl' % (d, setting['filename']), 'rb'))\n",
    "    mksg = pickle.load(open('mksg_by_mu_d_%d_%s.pkl' % (d, setting['filename']), 'rb'))\n",
    "    results = [irf, ksg, mksg, uf]\n",
    "    \n",
    "    for j, algo in enumerate(algos):\n",
    "        # Plot the mean over trials as a solid line.\n",
    "        ax.plot(mus, \n",
    "                np.mean(results[j], axis = 1).flatten(), \n",
    "                label = algo['label'], \n",
    "                linewidth = 2, \n",
    "                color = algo['color'])\n",
    "    \n",
    "    truth = np.zeros(len(mus))\n",
    "    for i in range(len(mus)):\n",
    "        I_XY, H_X, H_Y = compute_mutual_info(d, **setting['kwargs'], mu = mus[i])\n",
    "        truth[i] = I_XY / min(H_X, H_Y)\n",
    "    ax.plot(mus, truth, label = 'Truth', linewidth = 2, color = 'black')\n",
    "\n",
    "    ax.set_xlabel(\"Effect Size\")\n",
    "    ax.set_ylabel(\"Estimated Normalized MI\")\n",
    "    \n",
    "    ax.set_ylim(bottom = -0.05)\n",
    "    ax.set_ylim(top = 1.05)\n",
    "    ax.set_xlim(left = np.amin(mus) - 0.05)\n",
    "    ax.set_xlim(right = np.amax(mus) + 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data.\n",
    "n = 10000\n",
    "mus = range(5)\n",
    "num_trials = 20\n",
    "frac_eval = 0.3\n",
    "# n_estimators = 500\n",
    "d1 = 1\n",
    "d2 = 40\n",
    "\n",
    "# Algorithms.\n",
    "algos = [\n",
    "    {\n",
    "#         'instance': CalibratedClassifierCV(base_estimator=RandomForestClassifier(n_estimators = n_estimators), \n",
    "#                                            method='isotonic', \n",
    "#                                            cv = 5),\n",
    "        'label': 'IRF',\n",
    "        'title': 'Isotonic Reg. Forest',\n",
    "        'color': \"#fdae61\",\n",
    "    },\n",
    "    {\n",
    "#         'instance': None,\n",
    "        'label': 'KSG',\n",
    "        'title': 'KSG',\n",
    "        'color': \"#1b9e77\",\n",
    "    },\n",
    "    {\n",
    "#         'instance': None,\n",
    "        'label': 'Mixed KSG',\n",
    "        'title': 'Mixed KSG',\n",
    "        'color': \"purple\",\n",
    "    },\n",
    "    {\n",
    "#         'instance': UncertaintyForest(n_estimators = n_estimators),\n",
    "        'label': 'UF',\n",
    "        'title': 'Uncertainty Forest',\n",
    "        'color': \"#F41711\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for setting in settings:\n",
    "#     mutual_info_by_mu(mus, n, d1, frac_eval, algos, num_trials, setting, parallel = parallel)\n",
    "#     mutual_info_by_mu(mus, n, d2, frac_eval, algos, num_trials, setting, parallel = parallel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for setting in settings:\n",
    "    get_mutual_info_vs_mu(n, d1, num_trials, mus, setting, frac_eval)\n",
    "    get_mutual_info_vs_mu(n, d2, num_trials, mus, setting, frac_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute true values for normalized mutual info.\n",
    "# def compute_truth(d, settings, mus):\n",
    "#     truth = np.zeros((len(settings), len(mus)))\n",
    "#     for s in range(len(settings)): \n",
    "#         kwargs = settings[s]['kwargs']\n",
    "#         for i in range(len(mus)):\n",
    "#             kwargs['mu'] = mus[i]\n",
    "#             truth[s, i] = compute_norm_mutual_info(d, **kwargs)[0]\n",
    "#     with open('truth_fig3_d_%d.pkl' % d, 'wb') as f:\n",
    "#         pickle.dump(truth, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute_truth(d1, settings, mus)\n",
    "# compute_truth(d2, settings, mus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_fig3(algos, d1, d2, settings):\n",
    "    sns.set(font_scale = 1.5)\n",
    "    sns.set_style(\"ticks\")\n",
    "    plt.rcParams[\"font.family\"] = \"sans-serif\"\n",
    "    fig, axes = plt.subplots(len(settings), 3, figsize = (15,20))\n",
    "    \n",
    "#     with open('truth_fig3_d_%d.pkl' % d1, 'rb') as f:\n",
    "#         truth_1 = pickle.load(f)\n",
    "#     with open('truth_fig3_d_%d.pkl' % d2, 'rb') as f:\n",
    "#         truth_2 = pickle.load(f)\n",
    "\n",
    "    for s, setting in enumerate(settings):\n",
    "        plot_setting(2000, setting, axes[s, 0])\n",
    "        plot_mutual_info_by_mu(setting, algos, d1, axes[s, 1])\n",
    "        plot_mutual_info_by_mu(setting, algos, d2, axes[s, 2])\n",
    "        \n",
    "    axes[0, 1].set_title('d = %d' % d1)\n",
    "    axes[0, 2].set_title('d = %d' % d2)\n",
    "    axes[0, 2].legend(loc = \"upper right\")\n",
    "\n",
    "    plt.yticks(fontsize = 20)\n",
    "    plt.xticks(fontsize = 20)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"fig3.pdf\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run settings cell first.\n",
    "plot_fig3(algos, d1, d2, settings[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
