{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "from uncertainty_forest.uncertainty_forest import UncertaintyForest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from tqdm.notebook import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "from scipy.stats import entropy, norm\n",
    "from scipy.integrate import quad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taken from Richard's \"Reprod Figure 2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ronak/miniconda3/envs/mgc/lib/python3.7/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.ensemble.forest module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.ensemble. Anything that cannot be imported from sklearn.ensemble is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble.forest import _generate_unsampled_indices\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import math\n",
    "\n",
    "def cef_estimate(X, y, n_estimators = 200, max_samples = .32, bootstrap = True, depth = 30, min_samples_leaf = 1, max_features = 1.):\n",
    "    model = BaggingClassifier(DecisionTreeClassifier(max_depth = depth, min_samples_leaf = min_samples_leaf, max_features = math.ceil(int(math.sqrt(X.shape[1])))), \n",
    "                              n_estimators = n_estimators, \n",
    "                              max_samples= max_samples, \n",
    "                              bootstrap = bootstrap)\n",
    "    model.fit(X, y)\n",
    "    class_counts = np.zeros((X.shape[0], model.n_classes_))\n",
    "    for idx, tree in enumerate(model): # RONAK EDIT\n",
    "        # get out of bag indicies\n",
    "        \n",
    "        # RONAK EDIT STARTS HERE ################ In newer sklearn, generate unsampled takes a positional argument.\n",
    "        #unsampled_indices = _generate_unsampled_indices(tree.random_state, len(X))\n",
    "        sampled_indices = model.estimators_samples_[idx]\n",
    "        unsampled_indices = np.delete(np.arange(0, len(X)), sampled_indices)\n",
    "        \n",
    "        # RONAK EDIT ENDS HERE ##################\n",
    "        \n",
    "        total_unsampled = len(unsampled_indices)\n",
    "        np.random.shuffle(unsampled_indices)\n",
    "        prob_indices, eval_indices = unsampled_indices[:total_unsampled//2], unsampled_indices[total_unsampled//2:]\n",
    "        # get all node counts\n",
    "        node_counts = tree.tree_.n_node_samples\n",
    "        # get probs for eval samples\n",
    "        posterior_class_counts = np.zeros((len(node_counts), model.n_classes_))\n",
    "        for prob_index in prob_indices:\n",
    "            posterior_class_counts[tree.apply(X[prob_index].reshape(1, -1)).item(), y[prob_index]] += 1\n",
    "        row_sums = posterior_class_counts.sum(axis=1)\n",
    "        row_sums[row_sums == 0] = 1\n",
    "        class_probs = (posterior_class_counts/row_sums[:, None])\n",
    "        \n",
    "        where_0 = np.argwhere(class_probs == 0)\n",
    "        for elem in where_0:\n",
    "            class_probs[elem[0], elem[1]] = 1/(2*row_sums[elem[0], None])\n",
    "        where_1 = np.argwhere(class_probs == 1)\n",
    "        for elem in where_1:\n",
    "            class_probs[elem[0], elem[1]] = 1 - 1/(2*row_sums[elem[0], None])\n",
    "        \n",
    "        class_probs.tolist()\n",
    "        partition_counts = np.asarray([node_counts[x] for x in tree.apply(X[eval_indices])])\n",
    "        # get probability for out of bag samples\n",
    "        eval_class_probs = [class_probs[x] for x in tree.apply(X[eval_indices])]\n",
    "        eval_class_probs = np.array(eval_class_probs)\n",
    "        # find total elements for out of bag samples\n",
    "        elems = np.multiply(eval_class_probs, partition_counts[:, np.newaxis])\n",
    "        # store counts for each x (repeat fhis for each tree)\n",
    "        class_counts[eval_indices] += elems\n",
    "    # calculate p(y|X = x) for all x's\n",
    "    probs = class_counts/class_counts.sum(axis = 1, keepdims = True)\n",
    "    entropies = -np.sum(np.log(probs)*probs, axis = 1)\n",
    "    # convert nan to 0\n",
    "    entropies = np.nan_to_num(entropies)\n",
    "    return np.mean(entropies)\n",
    "\n",
    "np.warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CART_estimate(X, y, n_trees = 300, bootstrap = True, depth = 30):\n",
    "    model = RandomForestClassifier(bootstrap = bootstrap, n_estimators =n_trees, max_depth = depth, max_features = math.ceil(int(math.sqrt(X.shape[1]))))\n",
    "    model.fit(X, y)\n",
    "    class_counts = np.zeros((X.shape[0], model.n_classes_))\n",
    "    for tree_in_forest in model:\n",
    "        # get number of training elements in each partition\n",
    "        node_counts = tree_in_forest.tree_.n_node_samples\n",
    "        # get counts for all x (x.length array)\n",
    "        partition_counts = np.asarray([node_counts[x] for x in tree_in_forest.apply(X)])\n",
    "        # get class probability for all x (x.length, n_classes)\n",
    "        class_probs = tree_in_forest.predict_proba(X)\n",
    "        # get elements by performing row wise multiplication\n",
    "        elems = np.multiply(class_probs, partition_counts[:, np.newaxis])\n",
    "        # update counts for that tree\n",
    "        class_counts += elems\n",
    "    probs = class_counts/class_counts.sum(axis=1, keepdims=True)\n",
    "    entropies = -np.sum(np.log(probs)*probs, axis = 1)\n",
    "    # convert nan to 0\n",
    "    entropies = np.nan_to_num(entropies)\n",
    "    return np.mean(entropies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate Data and Conditional Entropy Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(n, d, mu = 1):\n",
    "    n_1 = np.random.binomial(n, .5) # number of class 1\n",
    "    mean = np.zeros(d)\n",
    "    mean[0] = mu\n",
    "    X_1 = np.random.multivariate_normal(mean, np.eye(d), n_1)\n",
    "    \n",
    "    X = np.concatenate((X_1, np.random.multivariate_normal(-mean, np.eye(d), n - n_1)))\n",
    "    y = np.concatenate((np.repeat(1, n_1), np.repeat(0, n - n_1)))\n",
    "  \n",
    "    return X, y\n",
    "\n",
    "# def split_train_eval(X, y, frac_eval):\n",
    "    \n",
    "#     if frac_eval == 0:\n",
    "#         return X, y, [], []\n",
    "    \n",
    "#     n = len(y)\n",
    "#     n_eval = int(np.floor(frac_eval*n))\n",
    "#     eval_indices = np.random.choice(np.arange(n), size = n_eval, replace = False)\n",
    "#     X_eval = X[eval_indices, :]\n",
    "#     y_eval = y[eval_indices]\n",
    "#     X = np.delete(X, eval_indices, axis = 0)\n",
    "#     y = np.delete(y, eval_indices, axis = 0)\n",
    "    \n",
    "#     return X, y, X_eval, y_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def conditional_entropy_distribution(n, d, frac_eval, algos, num_trials, mu, parallel = False):\n",
    "#     # For each trial, generate data and compute conditional entropy for each algorithm.\n",
    "#     def worker(t):\n",
    "#         # X, y, X_eval = generate_data(n, d, frac_eval, mu = mu)\n",
    "#         X, y = generate_data(n, d, 0, mu = mu)\n",
    "#         ret = np.zeros(len(algos))\n",
    "#         for i in range(len(algos)):\n",
    "#             obj = algos[i]['instance']\n",
    "#             if algos[i]['label'] == 'UF':\n",
    "#                 n_estimators = obj['n_estimators']\n",
    "#                 ret[i] = cef_estimate(X, y, n_estimators = n_estimators, min_samples_leaf = int(np.log(len(X))))\n",
    "#             else:\n",
    "#                 X, y, X_eval, y_eval = split_train_eval(X, y, frac_eval)\n",
    "#                 obj.fit(X, y)\n",
    "#                 p = obj.predict_proba(X_eval)\n",
    "#                 ret[i] = np.mean(entropy(p.T, base = np.exp(1)))\n",
    "#         return ret\n",
    "    \n",
    "#     if parallel:\n",
    "#         predicted_cond_entropy = np.array(Parallel(n_jobs=-2)(delayed(worker)(t) for t in range(num_trials)))\n",
    "#     else:\n",
    "#         predicted_cond_entropy = np.zeros((num_trials, len(algos)))\n",
    "#         for t in tqdm(range(num_trials)):\n",
    "#             predicted_cond_entropy[t, :] = worker(t)\n",
    "            \n",
    "#     return predicted_cond_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute True Conditional Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_cond_entropy(mu, base = np.exp(1)):\n",
    "    def func(x):\n",
    "        p = 0.5 * norm.pdf(x, mu, 1) + 0.5 * norm.pdf(x, -mu, 1)\n",
    "        return -p * np.log(p) / np.log(base)\n",
    "    \n",
    "    H_X = quad(func, -20, 20)\n",
    "    H_XY = 0.5*(1.0 + np.log(2 * np.pi)) / np.log(base)\n",
    "    H_Y = np.log(2.0) / np.log(base)\n",
    "    # I_XY = H_X - H_XY = H_Y - H_YX\n",
    "    return H_Y - H_X[0] + H_XY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional Entropy versus Sample Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def conditional_entropy_by_n(sample_sizes, d, frac_eval, algos, num_trials, mu, parallel = False):\n",
    "#     # Repeat for all 'n', and save output in the 'algos' array.\n",
    "#     cond_entropy_range = np.zeros((len(sample_sizes), num_trials, len(algos)))\n",
    "#     for i in range(len(sample_sizes)):\n",
    "#         cond_entropy_range[i, :, :] = conditional_entropy_distribution(sample_sizes[i], \n",
    "#                                                                        d, \n",
    "#                                                                        frac_eval, \n",
    "#                                                                        algos, \n",
    "#                                                                        num_trials, \n",
    "#                                                                        mu = mu,\n",
    "#                                                                        parallel = parallel)\n",
    "        \n",
    "#     for j in range(len(algos)):\n",
    "#         algos[j]['cond_entropy_by_n_d_%d' % d] = cond_entropy_range[:, :, j]\n",
    "        \n",
    "#     with open('algos_fig2.pkl', 'wb') as f:\n",
    "#         pickle.dump(algos, f)\n",
    "#     with open('sample_sizes_d_%d.pkl' % d, 'wb') as f:\n",
    "#         pickle.dump(sample_sizes, f)\n",
    "        \n",
    "#     return algos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cond_entropy_by_n(ax, num_plotted_trials, d, mu):\n",
    "        \n",
    "    sample_sizes = pickle.load(open('sample_sizes_d_%d.pkl' % d, 'rb'))\n",
    "    uf = pickle.load(open('uf_by_n_d_%d.pkl' % d, 'rb'))\n",
    "    cart = pickle.load(open('cart_by_n_d_%d.pkl' % d, 'rb'))\n",
    "    irf = pickle.load(open('irf_by_n_d_%d.pkl' % d, 'rb'))\n",
    "    uf2 = pickle.load(open('uf2_by_n_d_%d.pkl' % d, 'rb'))\n",
    "    uf2 = pickle.load(open('hon_by_n_d_%d.pkl' % d, 'rb'))\n",
    "    results = [cart, irf, uf, uf2, hon]\n",
    "    \n",
    "    for j, algo in enumerate(algos):\n",
    "        # Plot the mean over trials as a solid line.\n",
    "        ax.plot(sample_sizes,\n",
    "                np.mean(results[j], axis = 1).flatten(), \n",
    "                label = algo['label'], \n",
    "                linewidth = 4, \n",
    "                color = algo['color'])\n",
    "        # Use transparent lines to show other trials.\n",
    "        for t in range(num_plotted_trials):\n",
    "            ax.plot(sample_sizes, \n",
    "                    results[j][:, t].flatten(),  \n",
    "                    linewidth = 2, \n",
    "                    color = algo['color'],\n",
    "                    alpha = 0.15)\n",
    "    \n",
    "    truth = true_cond_entropy(mu)\n",
    "    ax.axhline(y = truth, linestyle = '-', color = \"black\", label = \"Truth\")\n",
    "            \n",
    "    ax.set_xlabel(\"Sample Size\")\n",
    "    ax.set_ylabel(\"Estimated Conditional Entropy\")\n",
    "    ax.set_title(\"Effect Size = %.1f, d = %d\" % (mu, d))\n",
    "    ax.set_ylim(ymin = -0.05, ymax = 1.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional Entropy Estimates versus Effect Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def conditional_entropy_by_mu(mus, n, d, frac_eval, algos, num_trials, parallel = False):\n",
    "#     # Repeat for all 'mu', and save output in the 'algos' array.\n",
    "#     cond_entropy_range = np.zeros((len(mus), num_trials, len(algos)))\n",
    "#     for i in range(len(mus)):\n",
    "#         cond_entropy_range[i, :, :] = conditional_entropy_distribution(n, \n",
    "#                                                                        d, \n",
    "#                                                                        frac_eval, \n",
    "#                                                                        algos, \n",
    "#                                                                        num_trials, \n",
    "#                                                                        mu = mus[i],\n",
    "#                                                                        parallel = parallel)      \n",
    "#     for j in range(len(algos)):\n",
    "#         algos[j]['cond_entropy_by_mu_d_%d' % d] = cond_entropy_range[:, :, j]\n",
    "        \n",
    "#     with open('algos_fig2.pkl', 'wb') as f:\n",
    "#         pickle.dump(algos, f)\n",
    "#     with open('mus_fig2.pkl', 'wb') as f:\n",
    "#         pickle.dump(mus, f)\n",
    "        \n",
    "#     return algos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cond_entropy_by_mu(ax, d, n):\n",
    "    \n",
    "    mus = pickle.load(open('mus_d_%d.pkl' % d, 'rb'))\n",
    "    uf = pickle.load(open('uf_by_mu_d_%d.pkl' % d, 'rb'))\n",
    "    cart = pickle.load(open('cart_by_mu_d_%d.pkl' % d, 'rb'))\n",
    "    irf = pickle.load(open('irf_by_mu_d_%d.pkl' % d, 'rb'))\n",
    "    uf2 = pickle.load(open('uf2_by_mu_d_%d.pkl' % d, 'rb'))\n",
    "    hon = pickle.load(open('hon_by_mu_d_%d.pkl' % d, 'rb'))\n",
    "    results = [cart, irf, uf, uf2, hon]\n",
    "        \n",
    "    for j, algo in enumerate(algos):\n",
    "        # Plot the mean over trials as a solid line.\n",
    "        ax.plot(mus, \n",
    "                np.mean(results[j], axis = 1).flatten(), \n",
    "                label = algo['label'], \n",
    "                linewidth = 4, \n",
    "                color = algo['color'])\n",
    "    \n",
    "    truth = [true_cond_entropy(mu) for mu in mus]\n",
    "    ax.plot(mus, truth, label = 'Truth', linewidth = 4, color = 'black')\n",
    "\n",
    "    ax.set_ylim(ymin = -.05)\n",
    "    ax.set_title(\"n = %d, d = %d\" % (n, d))\n",
    "    ax.set_xlabel(\"Effect Size\")\n",
    "    ax.set_ylabel(\"Estimated Conditional Entropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_fig2(num_plotted_trials, d1, d2, n1, n2, effect_size):\n",
    "    sns.set(font_scale = 3)\n",
    "    sns.set_style(\"ticks\")\n",
    "    plt.rcParams[\"font.family\"] = \"sans-serif\"\n",
    "    plt.rcParams['figure.figsize'] = [30, 20]\n",
    "    fig, axes = plt.subplots(2, 2)\n",
    "    \n",
    "    plot_cond_entropy_by_n(axes[0, 0], num_plotted_trials, d1, effect_size)\n",
    "    plot_cond_entropy_by_n(axes[0, 1], num_plotted_trials, d2, effect_size)\n",
    "                                                  \n",
    "    plot_cond_entropy_by_mu(axes[1, 0], d1, n1)\n",
    "    plot_cond_entropy_by_mu(axes[1, 1], d2, n2)\n",
    "    \n",
    "    axes[0,0].legend(loc = \"upper left\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"fig2.pdf\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_cond_entropy_vs_n(mean, d, num_trials, sample_sizes, algos):\n",
    "    \n",
    "# #     uf = np.zeros((len(sample_sizes), num_trials))\n",
    "# #     cart = np.zeros((len(sample_sizes), num_trials))\n",
    "# #     irf = np.zeros((len(sample_sizes), num_trials))\n",
    "# #     uf2 = np.zeros((len(sample_sizes), num_trials))\n",
    "#     hon = np.zeros((len(sample_sizes), num_trials))\n",
    "    \n",
    "#     def worker(t):\n",
    "#         # X, y = get_multivariate_sample(elem, d, mean)\n",
    "#         X, y = generate_data(elem, d, mu = mean)\n",
    "        \n",
    "# #         uf_out = cef_estimate(np.array(X), y, 300, .32, depth = 30)\n",
    "# #         cart_out = CART_estimate(X, y)\n",
    "        \n",
    "# #         irf_obj = CalibratedClassifierCV(base_estimator=RandomForestClassifier(n_estimators = 300), \n",
    "# #                                      method='isotonic', \n",
    "# #                                      cv = 5)\n",
    "# #         X, y, X_eval, y_eval = split_train_eval(X, y, frac_eval)\n",
    "# #         irf_obj.fit(X, y)\n",
    "# #         p = irf_obj.predict_proba(X_eval)\n",
    "# #         irf_out = np.mean(entropy(p.T, base = np.exp(1)))\n",
    "# #         uf2_out = UncertaintyForest(n_estimators = 300, frac_struct = 0.33).fit(X, y).estimate_cond_entropy()\n",
    "#         hon_out = UncertaintyForest(n_estimators = 300, frac_struct = 0.33, finite_correction = False).fit(X, y).estimate_cond_entropy()\n",
    "        \n",
    "# #         return (uf_out, cart_out, irf_out)\n",
    "#         return hon_out\n",
    "    \n",
    "#     for i, elem in enumerate(sample_sizes):\n",
    "#         output = np.array(Parallel(n_jobs=-2)(delayed(worker)(t) for t in range(num_trials)))\n",
    "# #         uf[i, :] = output[:, 0]\n",
    "# #         cart[i, :] = output[:, 1]\n",
    "# #         irf[i, :] = output[:, 2]\n",
    "# #         uf2[i, :] = output\n",
    "#         hon[i, :] = output\n",
    "        \n",
    "# #     pickle.dump(sample_sizes, open('sample_sizes_d_%d.pkl' % d, 'wb'))\n",
    "# #     pickle.dump(uf, open('uf_by_n_d_%d.pkl' % d, 'wb'))\n",
    "# #     pickle.dump(cart, open('cart_by_n_d_%d.pkl' % d, 'wb'))\n",
    "# #     pickle.dump(irf, open('irf_by_n_d_%d.pkl' % d, 'wb'))\n",
    "# #     pickle.dump(uf2, open('uf2_by_n_d_%d.pkl' % d, 'wb'))\n",
    "#     pickle.dump(hon, open('hon_by_n_d_%d.pkl' % d, 'wb'))\n",
    "\n",
    "#     return hon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cond_entropy_vs_n(mean, d, num_trials, sample_sizes, algos):\n",
    "    \n",
    "    # labels = [\"CART\", \"IRF\", \"UF1\", \"UF2\", \"UF3\", \"UF4\", \"UF5\"]\n",
    "    \n",
    "    def worker(t):\n",
    "        X, y = generate_data(elem, d, mu = mean)\n",
    "        \n",
    "        ret = []\n",
    "        for algo in algos:\n",
    "            ret.append(estimate_cef(X, y, algo['label']))\n",
    "\n",
    "        return tuple(ret)\n",
    "    \n",
    "    output = [np.zeros((len(sample_sizes), num_trials))]*len(algos)\n",
    "    for i, elem in enumerate(sample_sizes):\n",
    "        results = np.array(Parallel(n_jobs=-2)(delayed(worker)(t) for t in range(num_trials)))\n",
    "        for j in range(len(algos)):\n",
    "            output[j][i, :] = results[:, j]\n",
    "        \n",
    "    pickle.dump(sample_sizes, open('sample_sizes_d_%d.pkl' % d, 'wb'))\n",
    "    for j, algo in enumerate(algos):\n",
    "        pickle.dump(output[j], open('%s_by_n_d_%d.pkl' % (algo['label'], d), 'wb'))\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_cef(X, y, label):\n",
    "    if label == \"CART\":\n",
    "        return CART_estimate(X, y)\n",
    "    elif label == \"IRF\":\n",
    "        frac_eval = 0.3\n",
    "        irf = CalibratedClassifierCV(base_estimator=RandomForestClassifier(n_estimators = 300), \n",
    "                                     method='isotonic', \n",
    "                                     cv = 5)\n",
    "        X_train, y_train, X_eval, y_eval = split_train_eval(X, y, frac_eval)\n",
    "        irf.fit(X_train, y_train)\n",
    "        p = irf.predict_proba(X_eval)\n",
    "        return np.mean(entropy(p.T, base = np.exp(1)))\n",
    "    elif label == \"UF1\":\n",
    "        return cef_estimate(np.array(X), y, 300, .32, depth = 30)\n",
    "    elif label == \"UF2\":\n",
    "        uf = UncertaintyForest(n_estimators = 300, frac_struct = 0.33, frac_est = 0.33)\n",
    "        return uf.fit(X,y).estimate_cond_entropy()\n",
    "    elif label == \"UF3\":\n",
    "        uf = UncertaintyForest(n_estimators = 300, frac_struct = 0.33, frac_est = 0.33, finite_correction = False)\n",
    "        return uf.fit(X,y).estimate_cond_entropy()\n",
    "    elif label == \"UF4\":\n",
    "        uf = UncertaintyForest(n_estimators = 300, frac_struct = 0.50, frac_est = 0.25)\n",
    "        return uf.fit(X,y).estimate_cond_entropy()\n",
    "    elif label == \"UF5\":\n",
    "        uf = UncertaintyForest(n_estimators = 300, frac_struct = 0.25, frac_est = 0.50)\n",
    "        return uf.fit(X,y).estimate_cond_entropy()\n",
    "    else:\n",
    "        raise ValueError(\"Unrecognized Label!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_cond_entropy_vs_mu(n, d, num_trials, mus):\n",
    "    \n",
    "# #     uf = np.zeros((len(mus), num_trials))\n",
    "# #     cart = np.zeros((len(mus), num_trials))\n",
    "# #     irf = np.zeros((len(mus), num_trials))\n",
    "# #     uf2 = np.zeros((len(mus), num_trials))\n",
    "#     hon = np.zeros((len(mus), num_trials))\n",
    "    \n",
    "#     def worker(t):\n",
    "#         # X, y = get_multivariate_sample(elem, d, mean)\n",
    "#         X, y = generate_data(n, d, mu = elem)\n",
    "        \n",
    "# #         uf_out = cef_estimate(np.array(X), y, 300, .32, depth = 30)\n",
    "# #         cart_out = CART_estimate(X, y)\n",
    "        \n",
    "# #         irf_obj = CalibratedClassifierCV(base_estimator=RandomForestClassifier(n_estimators = 300), \n",
    "# #                                      method='isotonic', \n",
    "# #                                      cv = 5)\n",
    "# #         X, y, X_eval, y_eval = split_train_eval(X, y, frac_eval)\n",
    "# #         irf_obj.fit(X, y)\n",
    "# #         p = irf_obj.predict_proba(X_eval)\n",
    "# #         irf_out = np.mean(entropy(p.T, base = np.exp(1)))\n",
    "# #         uf2_out = UncertaintyForest(n_estimators = 300, frac_struct = 0.33).fit(X, y).estimate_cond_entropy()\n",
    "#         hon_out = UncertaintyForest(n_estimators = 300, frac_struct = 0.33).fit(X, y).estimate_cond_entropy()\n",
    "        \n",
    "#         # return (uf_out, cart_out, irf_out)\n",
    "#         return hon_out\n",
    "    \n",
    "#     for i, elem in enumerate(mus):\n",
    "#         output = np.array(Parallel(n_jobs=-2)(delayed(worker)(t) for t in range(num_trials)))\n",
    "# #         uf[i, :] = output[:, 0]\n",
    "# #         cart[i, :] = output[:, 1]\n",
    "# #         irf[i, :] = output[:, 2]\n",
    "# #         uf2[i, :] = output\n",
    "#         hon[i, :] = output\n",
    "        \n",
    "#     pickle.dump(mus, open('mus_d_%d.pkl' % d, 'wb'))\n",
    "# #     pickle.dump(uf, open('uf_by_mu_d_%d.pkl' % d, 'wb'))\n",
    "# #     pickle.dump(cart, open('cart_by_mu_d_%d.pkl' % d, 'wb'))\n",
    "# #     pickle.dump(irf, open('irf_by_mu_d_%d.pkl' % d, 'wb'))\n",
    "# #     pickle.dump(uf2, open('uf2_by_mu_d_%d.pkl' % d, 'wb'))\n",
    "#     pickle.dump(hon, open('hon_by_mu_d_%d.pkl' % d, 'wb'))\n",
    "\n",
    "#     # return uf, cart, irf\n",
    "#     return hon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cond_entropy_vs_mu(n, d, num_trials, mus, algos):\n",
    "    \n",
    "    # labels = [\"CART\", \"IRF\", \"UF1\", \"UF2\", \"UF3\", \"UF4\", \"UF5\"]\n",
    "    \n",
    "    def worker(t):\n",
    "        X, y = generate_data(n, d, mu = elem)\n",
    "        \n",
    "        ret = []\n",
    "        for algo in algos:\n",
    "            ret.append(estimate_cef(X, y, algo['label']))\n",
    "\n",
    "        return tuple(ret)\n",
    "    \n",
    "    output = [np.zeros((len(mus), num_trials))]*len(algos)\n",
    "    for i, elem in enumerate(mus):\n",
    "        results = np.array(Parallel(n_jobs=-2)(delayed(worker)(t) for t in range(num_trials)))\n",
    "        for j in range(len(algos)):\n",
    "            output[j][i, :] = results[:, j]\n",
    "        \n",
    "    pickle.dump(mus, open('mus.pkl', 'wb'))\n",
    "    for j, algo in enumerate(algos):\n",
    "        pickle.dump(output[j], open('%s_by_n_d_%d.pkl' % (algo['label'], d), 'wb'))\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Experiments and Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data.\n",
    "mus = [i * 0.5 for i in range(1, 11)]\n",
    "frac_eval = 0.3\n",
    "effect_size = 1\n",
    "d1 = 1\n",
    "d2 = 40\n",
    "n1 = 5000\n",
    "n2 = 10000\n",
    "num_trials = 20\n",
    "num_plotted_trials = 10\n",
    "sample_sizes_d1 = range(400, 2501, 300)\n",
    "sample_sizes_d2 = range(400, 10001, 960)\n",
    "\n",
    "# Fake params.\n",
    "# mus = [i * 0.5 for i in range(1, 4)]\n",
    "# # frac_eval = 0.3\n",
    "# # n_estimators = 200\n",
    "# effect_size = 1\n",
    "# d1 = 1\n",
    "# d2 = 3\n",
    "# n1 = 100\n",
    "# n2 = 110\n",
    "# num_trials = 3\n",
    "# num_plotted_trials = 3\n",
    "# sample_sizes_d1 = range(100, 120, 10)\n",
    "# sample_sizes_d2 = range(100, 130, 10)\n",
    "\n",
    "# Algorithms.\n",
    "algos = [\n",
    "    {\n",
    "        'label': 'CART',\n",
    "        'title': 'CART Forest',\n",
    "        'color': \"#1b9e77\",\n",
    "    },\n",
    "    {\n",
    "        'label': 'IRF',\n",
    "        'title': 'Isotonic Reg. Forest',\n",
    "        'color': \"#fdae61\",\n",
    "    },\n",
    "    {\n",
    "        'label': 'UF1',\n",
    "        'title': 'Uncertainty Forest 1',\n",
    "        'color': \"#F41711\",\n",
    "    },\n",
    "    {\n",
    "        'label': 'UF2',\n",
    "        'title': 'Uncertainty Forest 2',\n",
    "        'color': \"purple\",\n",
    "    },\n",
    "    {\n",
    "        'label': 'UF3',\n",
    "        'title': 'Uncertainty Forest 3',\n",
    "        'color': \"blue\",\n",
    "    },\n",
    "    {\n",
    "        'label': 'UF4',\n",
    "        'title': 'Uncertainty Forest 4',\n",
    "        'color': \"m\",\n",
    "    },\n",
    "    {\n",
    "        'label': 'UF5',\n",
    "        'title': 'Uncertainty Forest 5',\n",
    "        'color': \"g\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimated H(Y | X) versus n, d = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.39144704, 0.36690629, 0.41621105],\n",
       "        [0.44419516, 0.44632514, 0.44556528]])]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_cond_entropy_vs_n(effect_size, d1, num_trials, sample_sizes_d1, algos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimated H(Y | X) versus mu, d = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.57529053, 0.61522667, 0.56286711],\n",
       "        [0.42234329, 0.41702537, 0.35224401],\n",
       "        [0.2785826 , 0.24693697, 0.30674896]])]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Estimate conditional entropy vs mu.\n",
    "# algos = conditional_entropy_by_mu(mus, n1, d1, frac_eval, algos, num_trials_mu, parallel = parallel)\n",
    "# print(algos[0]['cond_entropy_by_mu_d_%d' % d1])\n",
    "get_cond_entropy_vs_mu(n1, d1, num_trials, mus, algos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimated H(Y | X) versus n, d = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.51000153, 0.53443465, 0.53206504],\n",
       "        [0.49126459, 0.47833288, 0.56313612],\n",
       "        [0.47465818, 0.49884061, 0.49159342]])]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Estimate conditional entropy vs n.\n",
    "# algos =conditional_entropy_by_n(sample_sizes_d2, d2, frac_eval, algos, num_trials_n, effect_size, parallel = parallel)\n",
    "# print(algos[0]['cond_entropy_by_n_d_%d' % d2])\n",
    "\n",
    "get_cond_entropy_vs_n(effect_size, d2, num_trials, sample_sizes_d2, algos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimated H(Y | X) versus mu, d = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.61595096, 0.59972681, 0.58682122],\n",
       "        [0.47451087, 0.5001933 , 0.50110811],\n",
       "        [0.40221417, 0.39922457, 0.35577567]])]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Estimate conditional entropy vs mu.\n",
    "# algos = conditional_entropy_by_mu(mus, n2, d2, frac_eval, algos, num_trials_mu, parallel = parallel)\n",
    "# print(algos[0]['cond_entropy_by_mu_d_%d' % d2])\n",
    "get_cond_entropy_vs_mu(n2, d2, num_trials, mus, algos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Correct log base.\n",
    "# old_base = 2.0\n",
    "# new_base = np.exp(1)\n",
    "# correction = np.log(old_base) / np.log(new_base)\n",
    "\n",
    "# for d in [d1, d2]:\n",
    "#     for label in ['uf2']:\n",
    "#         by_n = pickle.load(open('%s_by_n_d_%d.pkl' % (label, d), 'rb')) * correction\n",
    "#         pickle.dump(by_n, open('%s_by_n_d_%d.pkl' % (label, d), 'wb'))\n",
    "#         by_mu = pickle.load(open('%s_by_mu_d_%d.pkl' % (label, d), 'rb')) * correction\n",
    "#         pickle.dump(by_mu, open('%s_by_mu_d_%d.pkl' % (label, d), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_fig2(num_plotted_trials, d1, d2, n1, n2, effect_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
