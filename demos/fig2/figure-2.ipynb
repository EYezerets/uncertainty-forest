{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "from uncertainty_forest.uncertainty_forest import UncertaintyForest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from tqdm.notebook import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "from scipy.stats import entropy, norm\n",
    "from scipy.integrate import quad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate Data and Conditional Entropy Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(n, d, frac_eval, mu = 1):\n",
    "    n_1 = np.random.binomial(n, .5) # number of class 1\n",
    "    mean = np.zeros(d)\n",
    "    mean[0] = mu\n",
    "    X_1 = np.random.multivariate_normal(mean, np.eye(d), n_1)\n",
    "    \n",
    "    X = np.concatenate((X_1, np.random.multivariate_normal(-mean, np.eye(d), n - n_1)))\n",
    "    y = np.concatenate((np.repeat(1, n_1), np.repeat(-1, n - n_1)))\n",
    "    \n",
    "    # Evaluation data.\n",
    "    n_eval = int(np.floor(frac_eval*n))\n",
    "    eval_indices = np.random.choice(np.arange(n), size = n_eval, replace = False)\n",
    "    X_eval = X[eval_indices, :]\n",
    "    X = np.delete(X, eval_indices, axis = 0)\n",
    "    y = np.delete(y, eval_indices, axis = 0)\n",
    "    return X, y, X_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditional_entropy_distribution(n, d, frac_eval, algos, num_trials, mu, parallel = False):\n",
    "    # For each trial, generate data and compute conditional entropy for each algorithm.\n",
    "    def worker(t):\n",
    "        X, y, X_eval = generate_data(n, d, frac_eval, mu = mu)\n",
    "        ret = np.zeros(len(algos))\n",
    "        for i in range(len(algos)):\n",
    "            obj = algos[i]['instance']\n",
    "            obj.fit(X, y)\n",
    "            p = obj.predict_proba(X_eval)\n",
    "            ret[i] = np.mean(entropy(p.T, base = 2))\n",
    "        return ret\n",
    "    \n",
    "    if parallel:\n",
    "        predicted_cond_entropy = np.array(Parallel(n_jobs=-2)(delayed(worker)(t) for t in range(num_trials)))\n",
    "    else:\n",
    "        predicted_cond_entropy = np.zeros((num_trials, len(algos)))\n",
    "        for t in tqdm(range(num_trials)):\n",
    "            predicted_cond_entropy[t, :] = worker(t)\n",
    "            \n",
    "    return predicted_cond_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute True Conditional Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_cond_entropy(mu):\n",
    "    def func(x):\n",
    "        p = 0.5 * norm.pdf(x, mu, 1) + 0.5 * norm.pdf(x, -mu, 1)\n",
    "        return -p * np.log2(p)\n",
    "    \n",
    "    H_X = quad(func, -20, 20)\n",
    "    H_XY = 0.5*(1.0/np.log(2.0) + np.log2(2 * np.pi))\n",
    "    H_Y = 1\n",
    "    # I_XY = H_X - H_XY = H_Y - H_YX\n",
    "    return H_Y - H_X[0] + H_XY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional Entropy versus Sample Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditional_entropy_by_n(sample_sizes, d, frac_eval, algos, num_trials, mu, parallel = False):\n",
    "    # Repeat for all 'n', and save output in the 'algos' array.\n",
    "    cond_entropy_range = np.zeros((len(sample_sizes), num_trials, len(algos)))\n",
    "    for i in range(len(sample_sizes)):\n",
    "        cond_entropy_range[i, :, :] = conditional_entropy_distribution(sample_sizes[i], \n",
    "                                                                       d, \n",
    "                                                                       frac_eval, \n",
    "                                                                       algos, \n",
    "                                                                       num_trials, \n",
    "                                                                       mu = mu,\n",
    "                                                                       parallel = parallel)\n",
    "        \n",
    "    for j in range(len(algos)):\n",
    "        algos[j]['cond_entropy_by_n_d_%d' % d] = cond_entropy_range[:, :, j]\n",
    "        \n",
    "    with open('algos_fig2.pkl', 'wb') as f:\n",
    "        pickle.dump(algos, f)\n",
    "    with open('sample_sizes_d_%d.pkl' % d, 'wb') as f:\n",
    "        pickle.dump(sample_sizes, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cond_entropy_by_n(ax, num_plotted_trials, d, mu = 1):\n",
    "    with open('sample_sizes_d_%d.pkl' % d, 'rb') as f:\n",
    "        sample_sizes = pickle.load(f)\n",
    "    with open('algos_fig2.pkl', 'rb') as f:\n",
    "        algos = pickle.load(f)\n",
    "    \n",
    "    for algo in algos:\n",
    "        # Plot the mean over trials as a solid line.\n",
    "        ax.plot(sample_sizes,\n",
    "                np.mean(algo['cond_entropy_by_n_d_%d' % d], axis = 1).flatten(), \n",
    "                label = algo['label'], \n",
    "                linewidth = 4, \n",
    "                color = algo['color'])\n",
    "        # Use transparent lines to show other trials.\n",
    "        for t in range(num_plotted_trials):\n",
    "            ax.plot(sample_sizes, \n",
    "                    algo['cond_entropy_by_n_d_%d' % d][:, t].flatten(),  \n",
    "                    linewidth = 2, \n",
    "                    color = algo['color'],\n",
    "                    alpha = 0.15)\n",
    "    \n",
    "    truth = true_cond_entropy(mu)\n",
    "    ax.axhline(y = truth, linestyle = '-', color = \"black\", label = \"Truth\")\n",
    "            \n",
    "    ax.set_xlabel(\"Sample Size\")\n",
    "    ax.set_ylabel(\"Estimated Conditional Entropy\")\n",
    "    ax.set_title(\"Effect Size = %.1f, d = %d\" % (mu, d))\n",
    "    ax.set_ylim(ymin = -0.05, ymax = 1.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional Entropy Estimates versus Effect Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditional_entropy_by_mu(mus, n, d, frac_eval, algos, num_trials, parallel = False):\n",
    "    # Repeat for all 'mu', and save output in the 'algos' array.\n",
    "    cond_entropy_range = np.zeros((len(mus), num_trials, len(algos)))\n",
    "    for i in range(len(mus)):\n",
    "        cond_entropy_range[i, :, :] = conditional_entropy_distribution(n, \n",
    "                                                                       d, \n",
    "                                                                       frac_eval, \n",
    "                                                                       algos, \n",
    "                                                                       num_trials, \n",
    "                                                                       mu = mus[i],\n",
    "                                                                       parallel = parallel)      \n",
    "    for j in range(len(algos)):\n",
    "        algos[j]['cond_entropy_by_mu_d_%d' % d] = cond_entropy_range[:, :, j]\n",
    "        \n",
    "    with open('algos_fig2.pkl', 'wb') as f:\n",
    "        pickle.dump(algos, f)\n",
    "    with open('mus_fig2.pkl', 'wb') as f:\n",
    "        pickle.dump(mus, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cond_entropy_by_mu(ax, d, n):\n",
    "    with open('mus_fig2.pkl', 'rb') as f:\n",
    "        mus = pickle.load(f)\n",
    "    with open('algos_fig2.pkl', 'rb') as f:\n",
    "        algos = pickle.load(f)\n",
    "        \n",
    "    for algo in algos:\n",
    "        # Plot the mean over trials as a solid line.\n",
    "        ax.plot(mus, \n",
    "                np.mean(algo['cond_entropy_by_mu_d_%d' % d], axis = 1).flatten(), \n",
    "                label = algo['label'], \n",
    "                linewidth = 4, \n",
    "                color = algo['color'])\n",
    "    \n",
    "    truth = [true_cond_entropy(mu) for mu in mus]\n",
    "    ax.plot(mus, truth, label = 'Truth', linewidth = 4, color = 'black')\n",
    "\n",
    "    ax.set_ylim(ymin = -.05)\n",
    "    ax.set_title(\"n = %d, d = %d\" % (n, d))\n",
    "    ax.set_xlabel(\"Effect Size\")\n",
    "    ax.set_ylabel(\"Estimated Conditional Entropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_fig2(num_plotted_trials, d1, d2, n1, n2):\n",
    "    sns.set(font_scale = 3)\n",
    "    sns.set_style(\"ticks\")\n",
    "    plt.rcParams[\"font.family\"] = \"sans-serif\"\n",
    "    plt.rcParams['figure.figsize'] = [30, 20]\n",
    "    fig, axes = plt.subplots(2, 2)\n",
    "    \n",
    "    plot_cond_entropy_by_n(axes[0, 0], num_plotted_trials, d1)\n",
    "    plot_cond_entropy_by_n(axes[0, 1], num_plotted_trials, d2)\n",
    "                                                  \n",
    "    plot_cond_entropy_by_mu(axes[1, 0], d1, n1)\n",
    "    plot_cond_entropy_by_mu(axes[1, 1], d2, n2)\n",
    "    \n",
    "    axes[0,0].legend(loc = \"upper left\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"fig2.pdf\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Experiments and Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data.\n",
    "mus = [i * 0.5 for i in range(1, 11)]\n",
    "frac_eval = 0.3\n",
    "n_estimators = 500\n",
    "d1 = 1\n",
    "d2 = 20\n",
    "n1 = 6000\n",
    "n2 = 10000\n",
    "num_trials_n = 100\n",
    "num_trials_mu = 25\n",
    "sample_sizes_d1 = range(100, 2501, 300)\n",
    "sample_sizes_d2 = range(100, 10001, 990)\n",
    "\n",
    "# Algorithms.\n",
    "algos = [\n",
    "    {\n",
    "        'instance': RandomForestClassifier(n_estimators = n_estimators),\n",
    "        'label': 'CART',\n",
    "        'title': 'CART Forest',\n",
    "        'color': \"#1b9e77\",\n",
    "    },\n",
    "    {\n",
    "        'instance': CalibratedClassifierCV(base_estimator=RandomForestClassifier(n_estimators = n_estimators), \n",
    "                                           method='isotonic', \n",
    "                                           cv = 5),\n",
    "        'label': 'IRF',\n",
    "        'title': 'Isotonic Reg. Forest',\n",
    "        'color': \"#fdae61\",\n",
    "    },\n",
    "    {\n",
    "        'instance': UncertaintyForest(max_samples = 0.5, min_samples_leaf = 6, n_estimators = n_estimators),\n",
    "        'label': 'UF',\n",
    "        'title': 'Uncertainty Forest',\n",
    "        'color': \"#F41711\",\n",
    "    },\n",
    "]\n",
    "\n",
    "# Other.\n",
    "num_plotted_trials = 10\n",
    "parallel = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimated H(Y | X) versus n, d = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.1286221  0.34603851 0.28960922 0.02406427 0.27468368 0.3070487\n",
      "  0.39584301 0.17675064 0.17758862 0.12032135]\n",
      " [0.35909548 0.2421663  0.39606436 0.21952789 0.24971243 0.15389806\n",
      "  0.13202145 0.3221266  0.13880579 0.09505257]]\n"
     ]
    }
   ],
   "source": [
    "# Estimate conditional entropy vs n.\n",
    "conditional_entropy_by_n(sample_sizes_d1, d1, frac_eval, algos, num_trials_n, 1, parallel = parallel)\n",
    "print(algos[0]['cond_entropy_by_n_d_%d' % d1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimated H(Y | X) versus mu, d = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate conditional entropy vs mu.\n",
    "conditional_entropy_by_mu(mus, n1, d1, frac_eval, algos, num_trials_mu, parallel = parallel)\n",
    "print(algos[0]['cond_entropy_by_mu_d_%d' % d1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate conditional entropy vs n.\n",
    "conditional_entropy_by_n(sample_sizes_d2, d2, frac_eval, algos, num_trials_n, 1, parallel = parallel)\n",
    "print(algos[0]['cond_entropy_by_n_d_%d' % d2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate conditional entropy vs mu.\n",
    "conditional_entropy_by_mu(mus, n2, d2, frac_eval, algos, num_trials_mu, parallel = parallel)\n",
    "print(algos[0]['cond_entropy_by_mu_d_%d' % d2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_fig2(num_plotted_trials, d1, d2, n1, n2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
