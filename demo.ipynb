{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uncertainty Forest (UF) Demo\n",
    "This demo provides the basic use cases of the `uf` algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from scipy.stats import entropy\n",
    "\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uf(X, y, n_estimators = 300, max_samples = .4, base = np.exp(1), kappa = 3):\n",
    "    \n",
    "    # Build forest with default parameters.\n",
    "    model = BaggingClassifier(DecisionTreeClassifier(), \n",
    "                              n_estimators=n_estimators, \n",
    "                              max_samples=max_samples, \n",
    "                              bootstrap=False)\n",
    "    model.fit(X, y)\n",
    "    n = X.shape[0]\n",
    "    K = model.n_classes_\n",
    "    _, y = np.unique(y, return_inverse=True)\n",
    "    \n",
    "    cond_entropy = 0\n",
    "    for tree_idx, tree in enumerate(model):\n",
    "        # Find the indices of the training set used for partition.\n",
    "        sampled_indices = model.estimators_samples_[tree_idx]\n",
    "        unsampled_indices = np.delete(np.arange(0,n), sampled_indices)\n",
    "        \n",
    "        # Randomly split the rest into voting and evaluation.\n",
    "        total_unsampled = len(unsampled_indices)\n",
    "        np.random.shuffle(unsampled_indices)\n",
    "        vote_indices = unsampled_indices[:total_unsampled//2]\n",
    "        eval_indices = unsampled_indices[total_unsampled//2:]\n",
    "        \n",
    "        # Store the posterior in a num_nodes-by-num_classes matrix.\n",
    "        # Posteriors in non-leaf cells will be zero everywhere\n",
    "        # and later changed to uniform.\n",
    "        node_counts = tree.tree_.n_node_samples\n",
    "        class_counts = np.zeros((len(node_counts), K))\n",
    "        est_nodes = tree.apply(X[vote_indices])\n",
    "        est_classes = y[vote_indices]\n",
    "        for i in range(len(est_nodes)):\n",
    "            class_counts[est_nodes[i], est_classes[i]] += 1\n",
    "        \n",
    "        row_sums = class_counts.sum(axis=1) # Total number of estimation points in each leaf.\n",
    "        row_sums[row_sums == 0] = 1 # Avoid divide by zero.\n",
    "        class_probs = class_counts / row_sums[:, None]\n",
    "        \n",
    "        # Make the nodes that have no estimation indices uniform.\n",
    "        # This includes non-leaf nodes, but that will not affect the estimate.\n",
    "        class_probs[np.argwhere(class_probs.sum(axis = 1) == 0)] = [1 / K]*K\n",
    "        \n",
    "        # Apply finite sample correction and renormalize.\n",
    "        where_0 = np.argwhere(class_probs == 0)\n",
    "        for elem in where_0:\n",
    "            class_probs[elem[0], elem[1]] = 1 / (kappa*class_counts.sum(axis = 1)[elem[0]])\n",
    "        row_sums = class_probs.sum(axis=1)\n",
    "        class_probs = class_probs / row_sums[:, None]\n",
    "        \n",
    "        # Place evaluation points in their corresponding leaf node.\n",
    "        # Store evaluation posterior in a num_eval-by-num_class matrix.\n",
    "        eval_class_probs = class_probs[tree.apply(X[eval_indices])]\n",
    "        # eval_class_probs = [class_probs[x] for x in tree.apply(X[eval_indices])]\n",
    "        eval_entropies = [entropy(posterior) for posterior in eval_class_probs]\n",
    "        cond_entropy += np.mean(eval_entropies)\n",
    "      \n",
    "    return cond_entropy / n_estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimate the Conditional Entropy of `Y` given `X`\n",
    "Use `X_train` and `y_train` to estimate the posterior and conditional entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 10)\n",
      "(3000,)\n"
     ]
    }
   ],
   "source": [
    "n_class = 1000\n",
    "d = 10\n",
    "classes = [-1, 0, 1]\n",
    "n = n_class*len(classes)\n",
    "\n",
    "X = np.array(np.concatenate([np.random.multivariate_normal(k*np.ones(d), np.eye(d), n_class) for k in classes]))\n",
    "y = np.array(np.concatenate([k*np.ones(n_class) for k in classes]))\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time with n = 3000 and 300 trees.\n",
      "25.883588075637817\n",
      "H(Y|X) = 0.5563671200406107\n"
     ]
    }
   ],
   "source": [
    "n_estimators = 300\n",
    "\n",
    "start_time = time.time()\n",
    "cond_entropy = uf(X, y, n_estimators = n_estimators)\n",
    "elapsed_time = time.time() - start_time\n",
    "print(\"Elapsed time with n =\", n, \"and\", n_estimators, \"trees.\")\n",
    "print(elapsed_time)\n",
    "\n",
    "print(\"H(Y|X) =\", cond_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimate the mutual information\n",
    "Use the MLE for entropy of `y`, and subtract the conditional entropy for the mutual information estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I(X; Y) = 0.539756\n"
     ]
    }
   ],
   "source": [
    "# Compute frequency distribution for y.\n",
    "_, counts = np.unique(y, return_counts=True)\n",
    "entropy_y = entropy(counts, base=np.exp(1))\n",
    "\n",
    "mutual_info = entropy_y - cond_entropy\n",
    "print(\"I(X; Y) = %f\" % mutual_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
