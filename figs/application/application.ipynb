{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib as mpl\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from scipy.stats import entropy\n",
    "\n",
    "import graspy as gp\n",
    "from graspy.plot import heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_heatmap(graph, labels, title):\n",
    "    cmap = mpl.cm.get_cmap('binary')\n",
    "#     center = 0\n",
    "#     vmin = 0\n",
    "#     vmax = 1\n",
    "#     norm = mpl.colors.Normalize(0, 1)\n",
    "#     cc = np.linspace(0.5, 1, 256)\n",
    "#     cmap = mpl.colors.ListedColormap(cmap(cc))\n",
    "\n",
    "    heatmap_kws = dict(\n",
    "        cbar=False,\n",
    "        font_scale=1.4,\n",
    "        vmin=0,\n",
    "        vmax=1,\n",
    "        inner_hier_labels=labels,\n",
    "        hier_label_fontsize=20,\n",
    "        cmap=cmap,\n",
    "        center=None,\n",
    "    )\n",
    "    side_label_kws = dict(labelpad=45, fontsize=24)\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(20, 16))\n",
    "\n",
    "    # SBM\n",
    "    heatmap(graph, ax=ax, **heatmap_kws)\n",
    "    ax.set_title(title, pad = 100, fontdict = {'fontsize' : 23})\n",
    "\n",
    "    # Add colorbar\n",
    "#     sm = plt.cm.ScalarMappable(cmap=cmap)\n",
    "#     sm.set_array(data)\n",
    "#     cbar = fig.colorbar(sm, ax=ax, fraction=0.046, pad=0.04)\n",
    "#     cbar.ax.tick_params(labelsize=16)\n",
    "\n",
    "    plt.savefig(\n",
    "        \"adj_matrix.pdf\",\n",
    "        facecolor=\"w\",\n",
    "        format=\"pdf\",\n",
    "        bbox_inches=\"tight\",\n",
    "    )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph, labels = gp.datasets.load_drosophila_right(return_labels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_heatmap(graph, labels, \"Drosophila Right Connectome\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neuronal features and label.\n",
    "data = pd.read_csv(\"vdf.csv\")\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess\n",
    "labels = data.type.to_list()\n",
    "unique_labels = np.unique(labels)\n",
    "\n",
    "data.age = pd.factorize(data.age)[0]\n",
    "data.v = pd.factorize(data.v)[0]\n",
    "data = data.fillna(0)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = [\"claw\", \"dist\", \"age\", \"cluster\"]\n",
    "X = np.array(data[col_names])\n",
    "y = np.array(pd.factorize(data.type)[0])\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp.plot.pairplot(X, labels=labels, col_names = col_names, title = \"Drosophila Right Features\")\n",
    "plt.savefig(\n",
    "    \"neuron_features.pdf\",\n",
    "    facecolor=\"w\",\n",
    "    format=\"pdf\",\n",
    "    bbox_inches=\"tight\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uf(X, y, n_estimators = 300, max_samples = .4, base = np.exp(1), kappa = 3):\n",
    "    \n",
    "    # Build forest with default parameters.\n",
    "    model = BaggingClassifier(DecisionTreeClassifier(), \n",
    "                              n_estimators=n_estimators, \n",
    "                              max_samples=max_samples, \n",
    "                              bootstrap=False)\n",
    "    model.fit(X, y)\n",
    "    n = X.shape[0]\n",
    "    K = model.n_classes_\n",
    "    _, y = np.unique(y, return_inverse=True)\n",
    "    \n",
    "    cond_entropy = 0\n",
    "    for tree_idx, tree in enumerate(model):\n",
    "        # Find the indices of the training set used for partition.\n",
    "        sampled_indices = model.estimators_samples_[tree_idx]\n",
    "        unsampled_indices = np.delete(np.arange(0,n), sampled_indices)\n",
    "        \n",
    "        # Randomly split the rest into voting and evaluation.\n",
    "        total_unsampled = len(unsampled_indices)\n",
    "        np.random.shuffle(unsampled_indices)\n",
    "        vote_indices = unsampled_indices[:total_unsampled//2]\n",
    "        eval_indices = unsampled_indices[total_unsampled//2:]\n",
    "        \n",
    "        # Store the posterior in a num_nodes-by-num_classes matrix.\n",
    "        # Posteriors in non-leaf cells will be zero everywhere\n",
    "        # and later changed to uniform.\n",
    "        node_counts = tree.tree_.n_node_samples\n",
    "        class_counts = np.zeros((len(node_counts), K))\n",
    "        est_nodes = tree.apply(X[vote_indices])\n",
    "        est_classes = y[vote_indices]\n",
    "        for i in range(len(est_nodes)):\n",
    "            class_counts[est_nodes[i], est_classes[i]] += 1\n",
    "        \n",
    "        row_sums = class_counts.sum(axis=1) # Total number of estimation points in each leaf.\n",
    "        row_sums[row_sums == 0] = 1 # Avoid divide by zero.\n",
    "        class_probs = class_counts / row_sums[:, None]\n",
    "        \n",
    "        # Make the nodes that have no estimation indices uniform.\n",
    "        # This includes non-leaf nodes, but that will not affect the estimate.\n",
    "        class_probs[np.argwhere(class_probs.sum(axis = 1) == 0)] = [1 / K]*K\n",
    "        \n",
    "        # Apply finite sample correction and renormalize.\n",
    "        where_0 = np.argwhere(class_probs == 0)\n",
    "        for elem in where_0:\n",
    "            class_probs[elem[0], elem[1]] = 1 / (kappa*class_counts.sum(axis = 1)[elem[0]])\n",
    "        row_sums = class_probs.sum(axis=1)\n",
    "        class_probs = class_probs / row_sums[:, None]\n",
    "        \n",
    "        # Place evaluation points in their corresponding leaf node.\n",
    "        # Store evaluation posterior in a num_eval-by-num_class matrix.\n",
    "        eval_class_probs = class_probs[tree.apply(X[eval_indices])]\n",
    "        # eval_class_probs = [class_probs[x] for x in tree.apply(X[eval_indices])]\n",
    "        eval_entropies = [entropy(posterior) for posterior in eval_class_probs]\n",
    "        cond_entropy += np.mean(eval_entropies)\n",
    "      \n",
    "    return cond_entropy / n_estimators\n",
    "\n",
    "def entropy_estimate(y, base = np.exp(1)):\n",
    "    _, counts = np.unique(y, return_counts=True)\n",
    "    return entropy(counts, base=base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def powerset(s):\n",
    "    x = len(s)\n",
    "    masks = [1 << i for i in range(x)]\n",
    "    for i in range(1, (1 << x) - 1):\n",
    "        yield [ss for mask, ss in zip(masks, s) if i & mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H_Y = entropy_estimate(y)\n",
    "H_YX = uf(X, y)\n",
    "I_XY = H_Y - H_YX\n",
    "print(H_Y)\n",
    "print(H_YX)\n",
    "print(I_XY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute MI and Conditional MI for each subset of features.\n",
    "in_features = []\n",
    "I_YX_in = []\n",
    "I_YX_out = []\n",
    "\n",
    "dim_indices = np.arange(X.shape[1])\n",
    "subsets = powerset(dim_indices)\n",
    "for in_indices in subsets:\n",
    "    \n",
    "    # Generate string names for each subset of features, i.e. s1 = \"claw, dist\".\n",
    "    in_list = np.array(col_names)[in_indices]\n",
    "    in_features.append(','.join(in_list))\n",
    "    \n",
    "    out_indices = np.delete(dim_indices, in_indices)\n",
    "    X_in = X[:, in_indices]\n",
    "    X_out = X[:, out_indices]\n",
    "    \n",
    "    # H(Y | X_in)\n",
    "    H_YX_in = uf(X_in, y)\n",
    "    # H_YX_in = np.random.normal()\n",
    "    \n",
    "    # I(Y, X_in)\n",
    "    I_YX_in.append(H_Y - H_YX_in)\n",
    "    \n",
    "    # I(Y, X_out | X_in)\n",
    "    I_YX_out.append(H_YX_in - H_YX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stitch it together in a dataframe.\n",
    "mi_dict = {\n",
    "    \"in_features\" : in_features,\n",
    "    \"I_YX_in\" : I_YX_in,\n",
    "    \"I_YX_out|X_in\" : I_YX_out,\n",
    "    \"I_XY\" : [I_XY]*(2**len(dim_indices) - 2)\n",
    "}\n",
    "\n",
    "mi_table = pd.DataFrame(mi_dict)\n",
    "mi_table.to_csv(\"mi_table.csv\", index = False)\n",
    "print(mi_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
