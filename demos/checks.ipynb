{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complicance and Other Checks\n",
    "This notebook ensures that the `UncertaintyForest` estimator is `sklearn`-compliant, and consistent with Richard's code. These checks will be incorporated into tests later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from uncertainty_forest.uncertainty_forest import UncertaintyForest\n",
    "from scipy.stats import norm\n",
    "from scipy.integrate import quad\n",
    "\n",
    "# Manual forest generation.\n",
    "from sklearn.ensemble.forest import _generate_unsampled_indices\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check that Conditional Entropy estimate is consistent with Richard's code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(n, d, mu = 1):\n",
    "    n_1 = np.random.binomial(n, .5) # number of class 1\n",
    "    mean = np.zeros(d)\n",
    "    mean[0] = mu\n",
    "    X_1 = np.random.multivariate_normal(mean, np.eye(d), n_1)\n",
    "    \n",
    "    X = np.concatenate((X_1, np.random.multivariate_normal(-mean, np.eye(d), n - n_1)))\n",
    "    y = np.concatenate((np.repeat(1, n_1), np.repeat(-1, n - n_1)))\n",
    "  \n",
    "    return X, y\n",
    "\n",
    "def split_train_eval(X, y, frac_eval):\n",
    "    \n",
    "    if frac_eval == 0:\n",
    "        return X, y, [], []\n",
    "    \n",
    "    n = len(y)\n",
    "    n_eval = int(np.floor(frac_eval*n))\n",
    "    eval_indices = np.random.choice(np.arange(n), size = n_eval, replace = False)\n",
    "    X_eval = X[eval_indices, :]\n",
    "    y_eval = y[eval_indices]\n",
    "    X = np.delete(X, eval_indices, axis = 0)\n",
    "    y = np.delete(y, eval_indices, axis = 0)\n",
    "    \n",
    "    return X, y, X_eval, y_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_cond_entropy(mu, base = 2.0):\n",
    "    def func(x):\n",
    "        p = 0.5 * norm.pdf(x, mu, 1) + 0.5 * norm.pdf(x, -mu, 1)\n",
    "        return -p * np.log(p) / np.log(base)\n",
    "    \n",
    "    H_X = quad(func, -20, 20)\n",
    "    H_XY = 0.5*(1.0 + np.log(2 * np.pi)) / np.log(base)\n",
    "    H_Y = np.log(2.0) / np.log(base)\n",
    "    # I_XY = H_X - H_XY = H_Y - H_YX\n",
    "    return H_Y - H_X[0] + H_XY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cef_estimate(X, y, n_estimators = 200, max_samples = .32, bootstrap = True, depth = 30, min_samples_leaf = 1, max_features = 1.):\n",
    "    model = BaggingClassifier(DecisionTreeClassifier(max_depth = depth, min_samples_leaf = min_samples_leaf, max_features = math.ceil(int(math.sqrt(X.shape[1])))),\n",
    "                              n_estimators = n_estimators,\n",
    "                              max_samples= max_samples,\n",
    "                              bootstrap = bootstrap)\n",
    "    model.fit(X, y)\n",
    "    class_counts = np.zeros((X.shape[0], model.n_classes_))\n",
    "    for tree in model:\n",
    "        # get out of bag indicies\n",
    "        unsampled_indices = _generate_unsampled_indices(tree.random_state, len(X))\n",
    "\n",
    "        total_unsampled = len(unsampled_indices)\n",
    "        np.random.shuffle(unsampled_indices)\n",
    "        prob_indices, eval_indices = unsampled_indices[:total_unsampled//2], unsampled_indices[total_unsampled//2:]\n",
    "        # get all node counts\n",
    "        node_counts = tree.tree_.n_node_samples\n",
    "        # get probs for eval samples\n",
    "        posterior_class_counts = np.zeros((len(node_counts), model.n_classes_))\n",
    "        for prob_index in prob_indices:\n",
    "            posterior_class_counts[tree.apply(X[prob_index].reshape(1, -1)).item(), y[prob_index]] += 1\n",
    "        row_sums = posterior_class_counts.sum(axis=1)\n",
    "        row_sums[row_sums == 0] = 1\n",
    "        class_probs = (posterior_class_counts/row_sums[:, None])\n",
    "\n",
    "        where_0 = np.argwhere(class_probs == 0)\n",
    "        for elem in where_0:\n",
    "            class_probs[elem[0], elem[1]] = 1/(2*row_sums[elem[0], None])\n",
    "        where_1 = np.argwhere(class_probs == 1)\n",
    "        for elem in where_1:\n",
    "            class_probs[elem[0], elem[1]] = 1 - 1/(2*row_sums[elem[0], None])\n",
    "\n",
    "        class_probs.tolist()\n",
    "        partition_counts = np.asarray([node_counts[x] for x in tree.apply(X[eval_indices])])\n",
    "        # get probability for out of bag samples\n",
    "        eval_class_probs = [class_probs[x] for x in tree.apply(X[eval_indices])]\n",
    "        eval_class_probs = np.array(eval_class_probs)\n",
    "        # find total elements for out of bag samples\n",
    "        elems = np.multiply(eval_class_probs, partition_counts[:, np.newaxis])\n",
    "        # store counts for each x (repeat fhis for each tree)\n",
    "        class_counts[eval_indices] += elems\n",
    "    # calculate p(y|X = x) for all x's\n",
    "    probs = class_counts/class_counts.sum(axis = 1, keepdims = True)\n",
    "    entropies = -np.sum(np.log(probs)*probs, axis = 1)\n",
    "    # convert nan to 0\n",
    "    entropies = np.nan_to_num(entropies)\n",
    "    return np.mean(entropies)\n",
    "\n",
    "def uncertainty_forests_estimate(X, y, n_estimators = 300, eval_split_size = .33,\n",
    "                                                  subsample_size = 0.5,\n",
    "                                                  depth = 30,\n",
    "                                                  min_samples_leaf = 1):\n",
    "    \"\"\"\n",
    "    Uncertainty forest algorithm for estimating conditional entropy.\n",
    "    The major steps are as follows:\n",
    "    1. Split samples into EVAL and TRAIN according to eval_split_size\n",
    "    2. Get subsample s from TRAIN samples according to subsample_size\n",
    "    3. Split s into two partitions: STRUCT and PROB\n",
    "    4. Train decision forest on STRUCT (this is just sklearn's fit function)\n",
    "    5. For each tree in forest:\n",
    "        i) Use PROB samples to compute probabilites in each leaf node\n",
    "        ii) Perform robust finite sampling on posteriors computed by PROB samples\n",
    "        iii) Use posterior to get probability that eval sample fell in leaf node\n",
    "        iv) Weight eval sample probabilities by number of samples in leaf node (get class counts)\n",
    "    6. Compute probabilities from class counts over entire forest\n",
    "    7. Use probabilities to compute conditional entropies\n",
    "    8. Return average conditional entropy over all eval samples\n",
    "\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X: array-like\n",
    "        Training samples.\n",
    "\n",
    "    y: list\n",
    "        Class labels for training samples.\n",
    "\n",
    "    n_estimators: int\n",
    "        Number of trees in forest.\n",
    "\n",
    "    eval_split_size: float between 0 and 1\n",
    "        Percent of samples to include in evaluation partition.\n",
    "\n",
    "    subsample_size: float between 0 and 1\n",
    "        Percent of non eval samples to use for a tree (subsample forms PROB and STRUCT samples).\n",
    "\n",
    "    depth: int\n",
    "        Maximum depth in tree.\n",
    "\n",
    "    min_samples_leaf\n",
    "        Minimum number of samples in node to be considered leaf.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Conditional entropy estimate\n",
    "    \"\"\"\n",
    "\n",
    "    # split samples into eval and non eval\n",
    "    # X_train, X_eval, y_train, _ = train_test_split(X, y, test_size=eval_split_size)\n",
    "    X_train, y_train, X_eval, y_eval = split_train_eval(X, y, eval_split_size) # Ronak version.\n",
    "\n",
    "    max_samples = int(X_train.shape[0]*subsample_size)\n",
    "    split_samples = max_samples // 2\n",
    "    model = BaggingClassifier(DecisionTreeClassifier(max_depth = depth,\n",
    "                              min_samples_leaf = min_samples_leaf,\n",
    "                              # set default maximum features to sqrt(total features)\n",
    "                              max_features = math.ceil(int(math.sqrt(X.shape[1])))),\n",
    "                              n_estimators = n_estimators,\n",
    "                              max_samples= split_samples,\n",
    "                              bootstrap = True)\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    # Matrix used to compute final estimate\n",
    "    # Each entry is total number of PROB nodes of class y that fell into\n",
    "    # the same leaf nodes that a x_eval sample fell into.\n",
    "    class_counts = np.zeros((X_eval.shape[0], model.n_classes_))\n",
    "    for tree in model:\n",
    "\n",
    "        # get total unused samples indices\n",
    "        unsampled_indices = _generate_unsampled_indices(tree.random_state, len(X))\n",
    "        # shuffle unused sample indices\n",
    "        np.random.shuffle(unsampled_indices)\n",
    "        # take split_samples amount as PROB indices (honest sampling)\n",
    "        prob_indices = unsampled_indices[:split_samples]\n",
    "\n",
    "        # get all node counts in tree\n",
    "        node_counts = tree.tree_.n_node_samples\n",
    "\n",
    "        # use PROB samples to compute posterior for tree\n",
    "\n",
    "        # NOTE: this considers probabilities for all nodes in the tree.\n",
    "        # However, only leaf nodes will have relevant values\n",
    "        posterior_class_counts = np.zeros((len(node_counts), model.n_classes_))\n",
    "        # for each leaf, count number of samples with each class label that fell into leaf\n",
    "        # NOTE: unused leaf nodes will have probability 0 for every class\n",
    "\n",
    "        for prob_index in prob_indices:\n",
    "            posterior_class_counts[tree.apply(X[prob_index].reshape(1, -1)).item(), y[prob_index]] += 1\n",
    "\n",
    "        # divide each leaf node by number of total samples that fell into leaf node\n",
    "        row_sums = posterior_class_counts.sum(axis=1)\n",
    "        # handle divide by 0 errors\n",
    "        row_sums[row_sums == 0] = 1\n",
    "        class_probs = (posterior_class_counts/row_sums[:, None])\n",
    "\n",
    "        # perform robust finite sampling\n",
    "        class_probs = robust_finite_sampling(class_probs, row_sums, model.n_classes_)\n",
    "\n",
    "        # Get probabilities from EVAL samples for final estimate\n",
    "        # Weigh the probailities by number of samples in leaf node\n",
    "\n",
    "        # Get leaf nodes counts for each sample in EVAL\n",
    "        partition_counts = np.asarray([node_counts[x] for x in tree.apply(X_eval)])\n",
    "        # get probability for eval samples\n",
    "        eval_class_probs = [class_probs[x] for x in tree.apply(X_eval)]\n",
    "        eval_class_probs = np.array(eval_class_probs)\n",
    "        # Get total elements\n",
    "        elems = np.multiply(eval_class_probs, partition_counts[:, np.newaxis])\n",
    "        # store counts for each x (repeat fhis for each tree)\n",
    "        class_counts += elems\n",
    "    # calculate p(y|X = x) for all x's\n",
    "    forest_probabilities = class_counts/class_counts.sum(axis = 1, keepdims = True)\n",
    "    entropies = -np.sum(np.log(forest_probabilities)*forest_probabilities, axis = 1)\n",
    "    # convert nan to 0\n",
    "    entropies = np.nan_to_num(entropies)\n",
    "    # return sample mean\n",
    "    return np.mean(entropies)\n",
    "\n",
    "\n",
    "def robust_finite_sampling(class_probs, row_sums, n_classes):\n",
    "    where_0 = np.argwhere(class_probs == 0)\n",
    "    for elem in where_0:\n",
    "        class_probs[elem[0], elem[1]] = 1/(n_classes*row_sums[elem[0], None])\n",
    "    where_1 = np.argwhere(class_probs == 1)\n",
    "    for elem in where_1:\n",
    "        class_probs[elem[0], elem[1]] = 1 - (n_classes - 1)/(n_classes*row_sums[elem[0], None])\n",
    "    return class_probs.tolist()\n",
    "\n",
    "np.warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = 1\n",
    "d = 1\n",
    "#sample_sizes = 2*np.array([100, 200, 300, 400])\n",
    "sample_sizes = [100]\n",
    "num_trials = 5\n",
    "n_estimators = 300\n",
    "base = np.exp(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample size: 100\n",
      "0.8\n",
      "STRUCT 1 67\n",
      "n 100\n",
      "STRUCT 67\n",
      "EST 10\n",
      "EVAL 23\n",
      "0.8\n",
      "STRUCT 1 66\n",
      "n 100\n",
      "STRUCT 66\n",
      "EST 10\n",
      "EVAL 24\n",
      "0.8\n",
      "STRUCT 1 56\n",
      "n 100\n",
      "STRUCT 56\n",
      "EST 10\n",
      "EVAL 34\n",
      "0.8\n",
      "STRUCT 1 64\n",
      "n 100\n",
      "STRUCT 64\n",
      "EST 10\n",
      "EVAL 26\n",
      "0.8\n",
      "STRUCT 1 60\n",
      "n 100\n",
      "STRUCT 60\n",
      "EST 10\n",
      "EVAL 30\n"
     ]
    }
   ],
   "source": [
    "richard_cef = np.zeros((len(sample_sizes), num_trials))\n",
    "richard_uf = np.zeros((len(sample_sizes), num_trials))\n",
    "ronak = np.zeros((len(sample_sizes), num_trials))\n",
    "\n",
    "for i, n in enumerate(sample_sizes):\n",
    "    print(\"Sample size:\", n)\n",
    "    for t in range(num_trials):\n",
    "        X, y = generate_data(n, d, mu = mu)\n",
    "        # richard_cef[i, t] = cef_estimate(X, y, n_estimators = n_estimators)\n",
    "        # richard_uf[i, t] = uncertainty_forests_estimate(X, y, n_estimators = n_estimators)\n",
    "\n",
    "        uf = UncertaintyForest(n_estimators = 1, \n",
    "                               frac_struct = 0.80, \n",
    "                               frac_est = 0.10, \n",
    "                               frac_eval = 0.10, \n",
    "                               base = base, \n",
    "                               parallel = False,\n",
    "                               bootstrap = True)\n",
    "        uf.fit(X, y)\n",
    "        ronak[i, t] = uf.estimate_cond_entropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(sample_sizes,\n",
    "         np.mean(ronak, axis = 1).flatten(), \n",
    "         label = 'Ronak', \n",
    "         linewidth = 4, \n",
    "         color = 'red')\n",
    "for t in range(num_trials):\n",
    "    plt.plot(sample_sizes, \n",
    "    ronak[:, t].flatten(),  \n",
    "    linewidth = 2, \n",
    "    color = 'red',\n",
    "    alpha = 0.15)\n",
    "plt.plot(sample_sizes,\n",
    "         np.mean(richard_cef, axis = 1).flatten(), \n",
    "         label = 'Richard CEF', \n",
    "         linewidth = 4, \n",
    "         color = 'blue')\n",
    "for t in range(num_trials):\n",
    "    plt.plot(sample_sizes, \n",
    "    richard_cef[:, t].flatten(),  \n",
    "    linewidth = 2, \n",
    "    color = 'blue',\n",
    "    alpha = 0.15)\n",
    "plt.plot(sample_sizes,\n",
    "         np.mean(richard_uf, axis = 1).flatten(), \n",
    "         label = 'Richard UF', \n",
    "         linewidth = 4, \n",
    "         color = 'green')\n",
    "for t in range(num_trials):\n",
    "    plt.plot(sample_sizes, \n",
    "    richard_uf[:, t].flatten(),  \n",
    "    linewidth = 2, \n",
    "    color = 'green',\n",
    "    alpha = 0.15)\n",
    "\n",
    "truth = true_cond_entropy(mu, base = np.exp(1))\n",
    "plt.axhline(y = truth, linestyle = '-', color = \"black\", label = \"Truth\")\n",
    "\n",
    "plt.xlabel(\"Sample Size\")\n",
    "plt.ylabel(\"Estimated Conditional Entropy\")\n",
    "plt.title(\"Effect Size = %.1f, d = %d\" % (mu, d))\n",
    "plt.legend(loc = \"upper right\")\n",
    "plt.ylim(ymin = -0.05, ymax = 1.05)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
