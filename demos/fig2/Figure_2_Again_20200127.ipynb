{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Another try at Figure 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_multivariate_sample(n, d, mean):\n",
    "    x_sample = []\n",
    "    y_sample = []\n",
    "    means = np.zeros(d)\n",
    "    means[0] = mean\n",
    "    for i in range(n):\n",
    "        y = np.random.binomial(1, .5)\n",
    "        if (y == 0):\n",
    "            x = np.random.multivariate_normal(-means, np.identity(d))\n",
    "        else:\n",
    "            x = np.random.multivariate_normal(means, np.identity(d))\n",
    "        x_sample.append(x.tolist())\n",
    "        y_sample.append(y)\n",
    "    return np.asarray(x_sample), np.asarray(y_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Richard's Original CEF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is Richard's original CEF function.  Don't mess with it.\n",
    "from sklearn.ensemble.forest import _generate_unsampled_indices\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import math\n",
    "\n",
    "def cef_estimate_richard(X, y, n_estimators = 200, max_samples = .32, bootstrap = True, depth = 30, min_samples_leaf = 1, max_features = 1.):\n",
    "    model = BaggingClassifier(DecisionTreeClassifier(max_depth = depth, min_samples_leaf = min_samples_leaf, max_features = math.ceil(int(math.sqrt(X.shape[1])))), \n",
    "                              n_estimators = n_estimators, \n",
    "                              max_samples= max_samples, \n",
    "                              bootstrap = bootstrap)\n",
    "    model.fit(X, y)\n",
    "    class_counts = np.zeros((X.shape[0], model.n_classes_))\n",
    "    for tree in model:\n",
    "        \n",
    "        # get out of bag indices.       \n",
    "        # Here's where we obtain unsampled indices.\n",
    "        unsampled_indices = _generate_unsampled_indices(tree.random_state, len(X), int((1 - max_samples)*len(X)))\n",
    "        # Done with unsampled indices.\n",
    "        \n",
    "        total_unsampled = len(unsampled_indices)\n",
    "        np.random.shuffle(unsampled_indices)\n",
    "        prob_indices, eval_indices = unsampled_indices[:total_unsampled//2], unsampled_indices[total_unsampled//2:]\n",
    "        # get all node counts\n",
    "        node_counts = tree.tree_.n_node_samples\n",
    "        # get probs for eval samples\n",
    "        posterior_class_counts = np.zeros((len(node_counts), model.n_classes_))\n",
    "        for prob_index in prob_indices:\n",
    "            posterior_class_counts[tree.apply(X[prob_index].reshape(1, -1)).item(), y[prob_index]] += 1\n",
    "        row_sums = posterior_class_counts.sum(axis=1)\n",
    "        row_sums[row_sums == 0] = 1\n",
    "        class_probs = (posterior_class_counts/row_sums[:, None])\n",
    "        \n",
    "        where_0 = np.argwhere(class_probs == 0)\n",
    "        for elem in where_0:\n",
    "            class_probs[elem[0], elem[1]] = 1/(2*row_sums[elem[0], None])\n",
    "        where_1 = np.argwhere(class_probs == 1)\n",
    "        for elem in where_1:\n",
    "            class_probs[elem[0], elem[1]] = 1 - 1/(2*row_sums[elem[0], None])\n",
    "        \n",
    "        class_probs.tolist()\n",
    "        partition_counts = np.asarray([node_counts[x] for x in tree.apply(X[eval_indices])])\n",
    "        # get probability for out of bag samples\n",
    "        eval_class_probs = [class_probs[x] for x in tree.apply(X[eval_indices])]\n",
    "        eval_class_probs = np.array(eval_class_probs)\n",
    "        # find total elements for out of bag samples\n",
    "        elems = np.multiply(eval_class_probs, partition_counts[:, np.newaxis])\n",
    "        # store counts for each x (repeat fhis for each tree)\n",
    "        class_counts[eval_indices] += elems\n",
    "    # calculate p(y|X = x) for all x's\n",
    "    probs = class_counts/class_counts.sum(axis = 1, keepdims = True)\n",
    "    entropies = -np.sum(np.log(probs)*probs, axis = 1)\n",
    "    # convert nan to 0\n",
    "    entropies = np.nan_to_num(entropies)\n",
    "    return np.mean(entropies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mike's Altered CEF 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All we're changing is how we get unsampled indices.  This avoids using the deprecated\n",
    "# _generate_unsampled_indices function, which we know yields some sampled indices.\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import math\n",
    "\n",
    "def cef_estimate_mike_1(X, y, n_estimators = 200, max_samples = .32, bootstrap = True, depth = 30, min_samples_leaf = 1, max_features = 1.):\n",
    "    model = BaggingClassifier(DecisionTreeClassifier(max_depth = depth, min_samples_leaf = min_samples_leaf, max_features = math.ceil(int(math.sqrt(X.shape[1])))), \n",
    "                              n_estimators = n_estimators, \n",
    "                              max_samples= max_samples, \n",
    "                              bootstrap = bootstrap)\n",
    "    model.fit(X, y)\n",
    "    class_counts = np.zeros((X.shape[0], model.n_classes_))\n",
    "    tree_idx = 0\n",
    "    for tree in model:\n",
    "        \n",
    "        # get out of bag indices.       \n",
    "        # Here's where we obtain unsampled indices.\n",
    "        # unsampled_indices = _generate_unsampled_indices(tree.random_state, len(X), int((1 - max_samples)*len(X)))\n",
    "        sampled_indices = model.estimators_samples_[tree_idx]\n",
    "        unsampled_indices = np.delete(np.arange(0,X.shape[0]), sampled_indices)\n",
    "        tree_idx = tree_idx + 1\n",
    "        # Done with unsampled indices.\n",
    "        \n",
    "        total_unsampled = len(unsampled_indices)\n",
    "        np.random.shuffle(unsampled_indices)\n",
    "        prob_indices, eval_indices = unsampled_indices[:total_unsampled//2], unsampled_indices[total_unsampled//2:]\n",
    "        # get all node counts\n",
    "        node_counts = tree.tree_.n_node_samples\n",
    "        # get probs for eval samples\n",
    "        posterior_class_counts = np.zeros((len(node_counts), model.n_classes_))\n",
    "        for prob_index in prob_indices:\n",
    "            posterior_class_counts[tree.apply(X[prob_index].reshape(1, -1)).item(), y[prob_index]] += 1\n",
    "        row_sums = posterior_class_counts.sum(axis=1)\n",
    "        row_sums[row_sums == 0] = 1\n",
    "        class_probs = (posterior_class_counts/row_sums[:, None])\n",
    "        \n",
    "        where_0 = np.argwhere(class_probs == 0)\n",
    "        for elem in where_0:\n",
    "            class_probs[elem[0], elem[1]] = 1/(2*row_sums[elem[0], None])\n",
    "        where_1 = np.argwhere(class_probs == 1)\n",
    "        for elem in where_1:\n",
    "            class_probs[elem[0], elem[1]] = 1 - 1/(2*row_sums[elem[0], None])\n",
    "        \n",
    "        class_probs.tolist()\n",
    "        partition_counts = np.asarray([node_counts[x] for x in tree.apply(X[eval_indices])])\n",
    "        # get probability for out of bag samples\n",
    "        eval_class_probs = [class_probs[x] for x in tree.apply(X[eval_indices])]\n",
    "        eval_class_probs = np.array(eval_class_probs)\n",
    "        # find total elements for out of bag samples\n",
    "        elems = np.multiply(eval_class_probs, partition_counts[:, np.newaxis])\n",
    "        # store counts for each x (repeat fhis for each tree)\n",
    "        class_counts[eval_indices] += elems\n",
    "    # calculate p(y|X = x) for all x's\n",
    "    probs = class_counts/class_counts.sum(axis = 1, keepdims = True)\n",
    "    entropies = -np.sum(np.log(probs)*probs, axis = 1)\n",
    "    # convert nan to 0\n",
    "    entropies = np.nan_to_num(entropies)\n",
    "    return np.mean(entropies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d = 1, mu = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "\n",
    "cef_r = []\n",
    "cef_m1 = []\n",
    "\n",
    "d = 1\n",
    "mean = 1\n",
    "\n",
    "sample_sizes = [1000, 2000, 4000, 8000, 16000, 32000, 64000, 96000]\n",
    "traces = 5\n",
    "\n",
    "for trace in range(traces):\n",
    "    print(trace)\n",
    "    np.random.seed(123*trace)\n",
    "    X_full, y_full = get_multivariate_sample(np.max(sample_sizes), d, mean)\n",
    "    for elem in tqdm_notebook(sample_sizes):\n",
    "        X = X_full[0:elem, :]\n",
    "        y = y_full[0:elem]\n",
    "        cef_r.append(cef_estimate_richard(np.array(X), y, 300, .32, depth = 30))\n",
    "        cef_m1.append(cef_estimate_mike_1(np.array(X), y, 300, .32, depth = 30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "file = open('figure_2_cef_r_d1_mean1_20200127.pkl', 'wb') \n",
    "pickle.dump(cef_r, file)\n",
    "file = open('figure_2_cef_m1_d1_mean1_20200127.pkl', 'wb') \n",
    "pickle.dump(cef_m1, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cef_r_traces = np.asarray(cef_r).reshape((traces,len(sample_sizes))).T\n",
    "cef_m1_traces = np.asarray(cef_m1).reshape((traces,len(sample_sizes))).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cef_r_mean = np.mean(cef_r_traces, axis = 1)\n",
    "cef_m1_mean = np.mean(cef_m1_traces, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "sns.set(font_scale = 2.5)\n",
    "sns.set_style(\"ticks\")\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.rcParams[\"font.family\"] = \"sans-serif\"\n",
    "plt.rcParams['figure.figsize'] = [15, 10]\n",
    "\n",
    "for trace in range(traces):\n",
    "    line_cef_r, = plt.plot(sample_sizes, cef_r_traces[:, trace], label = \"CEF_R\", linewidth = .5, linestyle = \"solid\", color = \"red\")\n",
    "    line_cef_m1, = plt.plot(sample_sizes, cef_m1_traces[:, trace], label = \"CEF_M1\", linewidth = .5, linestyle = \"dashdot\", color = \"blue\")\n",
    "line_true = plt.axhline(y=.356, linestyle='-', color = \"black\", label = \"Truth\")\n",
    "plt.plot(sample_sizes, cef_r_mean, linewidth = 2, linestyle = \"solid\", color = \"red\")\n",
    "plt.plot(sample_sizes, cef_m1_mean, linewidth = 2, linestyle = \"solid\", color = \"blue\")\n",
    "\n",
    "plt.ylim(ymin = 0)\n",
    "plt.ylim(ymax = .8)\n",
    "plt.legend(handles=[line_cef_r, line_cef_m1, line_true])\n",
    "plt.xlabel('n')\n",
    "plt.ylabel(\"$\\hat H(Y|X)$\")\n",
    "plt.tight_layout()\n",
    "plt.title(\"Conditional entropy estimate vs \\nincreasing n (d = 1, effect size = 1)\")\n",
    "plt.savefig(\"alg_comp_1d_mean1_20200127.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d = 40, mu = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "\n",
    "cef_r = []\n",
    "cef_m1 = []\n",
    "\n",
    "d = 40\n",
    "mean = 1\n",
    "\n",
    "sample_sizes = [i*10000 for i in np.arange(1, 11)]\n",
    "traces = 5\n",
    "\n",
    "for trace in range(traces):\n",
    "    print(trace)\n",
    "    np.random.seed(123*trace)\n",
    "    X_full, y_full = get_multivariate_sample(np.max(sample_sizes), d, mean)\n",
    "    for elem in tqdm_notebook(sample_sizes):\n",
    "        X = X_full[0:elem, :]\n",
    "        y = y_full[0:elem]\n",
    "        cef_r.append(cef_estimate_richard(np.array(X), y, 300, .32, depth = 30))\n",
    "        cef_m1.append(cef_estimate_mike_1(np.array(X), y, 300, .32, depth = 30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "file = open('figure_2_cef_r_d40_mean1_20200127.pkl', 'wb') \n",
    "pickle.dump(cef_r, file)\n",
    "file = open('figure_2_cef_m1_d40_mean1_20200127.pkl', 'wb') \n",
    "pickle.dump(cef_m1, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cef_r_traces = np.asarray(cef_r).reshape((traces,len(sample_sizes))).T\n",
    "cef_m1_traces = np.asarray(cef_m1).reshape((traces,len(sample_sizes))).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cef_r_mean = np.mean(cef_r_traces, axis = 1)\n",
    "cef_m1_mean = np.mean(cef_m1_traces, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "sns.set(font_scale = 2.5)\n",
    "sns.set_style(\"ticks\")\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.rcParams[\"font.family\"] = \"sans-serif\"\n",
    "plt.rcParams['figure.figsize'] = [15, 10]\n",
    "\n",
    "for trace in range(traces):\n",
    "    line_cef_r, = plt.plot(sample_sizes, cef_r_traces[:, trace], label = \"CEF_R\", linewidth = .5, linestyle = \"solid\", color = \"red\", alpha = .5)\n",
    "    line_cef_m1, = plt.plot(sample_sizes, cef_m1_traces[:, trace], label = \"CEF_M1\", linewidth = .5, linestyle = \"solid\", color = \"blue\", alpha = .5)\n",
    "line_true = plt.axhline(y=.356, linestyle='-', color = \"black\", label = \"Truth\")\n",
    "plt.plot(sample_sizes, cef_r_mean, linewidth = 2, linestyle = \"solid\", color = \"red\")\n",
    "plt.plot(sample_sizes, cef_m1_mean, linewidth = 2, linestyle = \"solid\", color = \"blue\")\n",
    "\n",
    "plt.ylim(ymin = 0)\n",
    "plt.ylim(ymax = .8)\n",
    "plt.legend(handles=[line_cef_r, line_cef_m1, line_true])\n",
    "plt.xlabel('n')\n",
    "plt.ylabel(\"$\\hat H(Y|X)$\")\n",
    "plt.tight_layout()\n",
    "plt.title(\"Conditional entropy estimate vs \\nincreasing n (d = 40, effect size = 1)\")\n",
    "plt.savefig(\"alg_comp_40d_mean1_20200127.png\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
